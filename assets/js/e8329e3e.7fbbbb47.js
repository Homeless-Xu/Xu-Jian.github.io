"use strict";(self.webpackChunkblog_never_del=self.webpackChunkblog_never_del||[]).push([[4327],{3905:(e,n,t)=>{t.d(n,{Zo:()=>i,kt:()=>m});var o=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,o,r=function(e,n){if(null==e)return{};var t,o,r={},a=Object.keys(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var p=o.createContext({}),c=function(e){var n=o.useContext(p),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},i=function(e){var n=c(e.components);return o.createElement(p.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},u=o.forwardRef((function(e,n){var t=e.components,r=e.mdxType,a=e.originalType,p=e.parentName,i=l(e,["components","mdxType","originalType","parentName"]),u=c(t),m=r,h=u["".concat(p,".").concat(m)]||u[m]||d[m]||a;return t?o.createElement(h,s(s({ref:n},i),{},{components:t})):o.createElement(h,s({ref:n},i))}));function m(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var a=t.length,s=new Array(a);s[0]=u;var l={};for(var p in n)hasOwnProperty.call(n,p)&&(l[p]=n[p]);l.originalType=e,l.mdxType="string"==typeof e?e:r,s[1]=l;for(var c=2;c<a;c++)s[c]=t[c];return o.createElement.apply(null,s)}return o.createElement.apply(null,t)}u.displayName="MDXCreateElement"},3512:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>s,default:()=>d,frontMatter:()=>a,metadata:()=>l,toc:()=>c});var o=t(7462),r=(t(7294),t(3905));const a={sidebar_position:1930,title:"\ud83c\udfaa-9\ufe0f\u20e3\ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c Use"},s="# Storage \u2736 CEPH \u279c Use",l={unversionedId:"\ud83c\udfaa-9\ufe0f\u20e3 \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c Use",id:"\ud83c\udfaa-9\ufe0f\u20e3 \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c Use",title:"\ud83c\udfaa-9\ufe0f\u20e3\ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c Use",description:"Storage \u2736 CEPH \u279c Use",source:"@site/docs/\ud83c\udfaa-9\ufe0f\u20e3 \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c Use.md",sourceDirName:".",slug:"/\ud83c\udfaa-9\ufe0f\u20e3 \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c Use",permalink:"/\ud83c\udfaa-9\ufe0f\u20e3 \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c Use",draft:!1,tags:[],version:"current",sidebarPosition:1930,frontMatter:{sidebar_position:1930,title:"\ud83c\udfaa-9\ufe0f\u20e3\ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c Use"},sidebar:"defaultSidebar",previous:{title:"\ud83c\udfaa-9\ufe0f\u20e3\ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c Build",permalink:"/\ud83c\udfaa-9\ufe0f\u20e3 \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c Build"},next:{title:"\ud83c\udfaa-9\ufe0f\u20e3\ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c\u279c Driver",permalink:"/\ud83c\udfaa-9\ufe0f\u20e3 \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c\u279c Driver"}},p={},c=[],i={toc:c};function d(e){let{components:n,...t}=e;return(0,r.kt)("wrapper",(0,o.Z)({},i,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"-storage--ceph--use"},"# Storage \u2736 CEPH \u279c Use"),(0,r.kt)("p",null,"\ud83d\udd35 Good File "),(0,r.kt)("p",null,"\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\n",(0,r.kt)("a",{parentName:"p",href:"https://zhangzhuo.ltd/articles/2021/06/18/1623993444076.html"},"https://zhangzhuo.ltd/articles/2021/06/18/1623993444076.html")),(0,r.kt)("p",null,"\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\n",(0,r.kt)("a",{parentName:"p",href:"https://www.koenli.com/ef5921b8.html"},"https://www.koenli.com/ef5921b8.html")),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://blog.51cto.com/renlixing/3134294"},"https://blog.51cto.com/renlixing/3134294")),(0,r.kt)("p",null,"\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35 Basic Info\n\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35 Basic Info\n\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35 Basic Info"),(0,r.kt)("p",null,"\ud83d\udd35 POOL PG PGP Desc  "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"\ud83d\udd36 Pool \n\n    ceph osd pool create poolname PG PGP \n    ceph osd pool create mytestpool 128 128 \n\n    \u521b\u5efapool \u65f6\u5019\u5fc5\u987b\u6307\u5b9a PG \u6570\u91cf.\n    PG \u6570\u91cf \u662f\u81ea\u52a8\u5206\u5e03\u5230 \u6240\u6709 OSD\u7684.\n    \u6240\u4ee5\u4f60\u7684 Pool \u4e5f\u662f\u5206\u5e03\u5230\u6574\u4e2a \u96c6\u7fa4\u7684.\n\n\n\n\ud83d\udd36 PGs Desc  \u2705\n\n    ceph cluster have lots physical disk.\n        to manager disk easyier. we create PGs ( assign physical disk to some group )\n            so we only need manage PGs.\n\n\n\n\ud83d\udd36 PG / PGP \n\n    PG \u8981\u6839\u636e OSD \u6570\u91cf\u8fdb\u884c\u8c03\u6574.\n    PGP \u548c PG \u7684\u503c\u4fdd\u6301\u4e00\u81f4\u5c31\u884c.\n    \u6539PG \u7684\u540c\u65f6\u8981 PGP \u4e00\u8d77\u6539.\n\n\n    \u5c11\u4e8e50 osd \u53ef\u4ee5\u7528\u81ea\u52a8\u65b9\u5f0f. \u518d\u591a\u5c31\u8981\u81ea\u5df1\u7b97\u4e86. \n        pg_autoscale_mode: on   \u5c31\u53ef\u4ee5\u4e86\n")),(0,r.kt)("p",null,"\ud83d\udd35 Storage Develop "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"DAS:  Dirtect Attached Storage \n    \u279c NAS:  Network Attached Storage \n        \u279c SAN:  Storage Area Network \n            \u279c Object Storage\n")),(0,r.kt)("p",null,"\ud83d\udd35 CEPH Function:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Object Storage \u279c like AWS S3    \u279c For Share              \u279c high speed \nBlock  Storage \u279c like iscsi     \u279c For One Device Only    \u279c Fast Speed \nFile   Storage \u279c like NFS/SMB   \u279c For Share              \u279c Slow Speed\n\n    object storage have both advantage of  block storage and file storage.\n        if data that don`t change too often.    \u279c Object storage is best.  like picture/video \n        if you install VM                       \u279c Block Storage is better.\n")),(0,r.kt)("p",null,"\ud83d\udd35 CEPH FIle  Storage Workflow"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"1. File   >> Object           cut big file to samll object(2M-4M)\n    2. Object >> PG           give small object a PGs.\n        3. PG     >> OSD      Write  small object to OSD\n")),(0,r.kt)("p",null,"\ud83d\udd35 NEW OSD "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"when add new driver.\nit will automatic move date between osdes. \n")),(0,r.kt)("p",null,"\ud83d\udd35 Pool Commands "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"\ud83d\udd36 Pool \n\n    \u25ce Create pool     ceph osd pool create CEPH_FS-APP 128 128 \n    \u25ce Rename Pool     ceph osd pool rename CEPH-Test CEPH_FS-NAS-Ceph\n    \u25ce List   pool     ceph osd lspools\n    \u25ce Delete Pool\n\n\n\n\n\ud83d\udd36 AutoScale Mode  \n\n    \u25ce Set Mode On    ceph osd pool set CEPH-Test pg_autoscale_mode on\n    \u25ce Check Mode     ceph osd pool autoscale-status\n        default: on \n\n\n\ud83d\udd36 Pool Size \n\n    ceph osd pool set CEPH-Test target_size_bytes 10G\n\n\n\n\ud83d\udd36 Pool replica size \n\n    ceph osd pool set CEPH-Test size 2\n    \n    default is 3 \n    you can change this anytime.\n    if you have space set 3/2\n    if no space set to 1 at any time.\n\n\n\ud83d\udd36 Delete Pool \u2705\n\n    \u25ce Allow Delete pool \n        by default it is not allowed.\n\n        ceph tell mon.\\* injectargs '--mon-allow-pool-delete=true'\n\n\n    \u25ce How Delete \n\n        ceph osd pool delete {pool-name} {pool-name} --yes-i-really-really-mean-it\n            type pool name twice. \n                and follew --yes-i-really-really-mean-it\n\n        ceph osd pool delete test-pool test-pool --yes-i-really-really-mean-it\n\n        ceph osd pool delete OB-Zone-Ark.rgw.meta OB-Zone-Ark.rgw.meta --yes-i-really-really-mean-it\n")),(0,r.kt)("p",null,"\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35 Filesystem Demo\n\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35 Filesystem Demo\n\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35 Filesystem Demo "),(0,r.kt)("p",null,"\ud83d\udd35 Server \u2736 Pool Config "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'\ud83d\udd36 Create Pool for cephfs \n\n    ceph osd pool create CEPH_FS-NAS-Ceph 128 128 \n    ceph osd pool create CEPH_FS-NAS-Ceph-metadata 128 128 \n\n\n\ud83d\udd36 Assign Pool Type to Filesystem Storage\n\n    \u25ce Desc \n        sudo ceph osd pool application enable PoolNAME StorageType\n            ceph have tree storage type: file Type /Block Devie Type /Object Type.\n            so you need set pool use which type.\n            here cephfs type.\n \n    \u25ce Demo \n        sudo ceph osd pool application enable CEPH_FS-NAS-Ceph cephfs\n        sudo ceph osd pool application enable CEPH_FS-NAS-Ceph-metadata cephfs\n\n\n\ud83d\udd36 Create fs:  cephfs\n\n    ceph fs new cephfs CEPH_FS-NAS-Ceph-metadata CEPH_FS-NAS-Ceph\n\n\n\ud83d\udd36 Check fs status\n\n    CEPH-MGR.Root ~ ceph fs ls\n    name: cephfs, metadata pool: CEPH_FS-NAS-Ceph-metadata, data pools: [CEPH_FS-NAS-Ceph ]\n\n\n\ud83d\udd36 assign cephfs to node\n\n    ceph orch apply mds cephfs --placement="1  CEPH-Node"\n\n\n\ud83d\udd36 check mds status \n\n    CEPH-MGR.Root ~ ceph orch ps --daemon-type mds\n    NAME                         HOST       STATUS         REFRESHED  AGE  VERSION  IMAGE NAME             IMAGE ID      CONTAINER ID\n    mds.cephfs.CEPH-Node.tevxqv  CEPH-Node  running (33m)  4m ago     33m  15.2.16  quay.io/ceph/ceph:v15  8d5775c85c6a  6e5f7903011d\n')),(0,r.kt)("p",null,"\ud83d\udd35 Server \u2736 User Create "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"\ud83d\udd36 Create user for ceph-fs mount\n\n    ceph auth get-or-create client.miranda mon 'allow r' mds 'allow r, allow rw path=/' osd 'allow rw pool=data' -o ceph.client.miranda.keyring\n\n             \u25ce username:  client.miranda\n\n        \u203c\ufe0f cd /etc/ceph/  firts.   user name must be like  client.xxxx /  only change miranda above. leave else \u203c\ufe0f \n        \u203c\ufe0f cd /etc/ceph/  firts.   user name must be like  client.xxxx /  only change miranda above. leave else \u203c\ufe0f \n\n\n\ud83d\udd36 cat user token (option)\n\n    ceph auth get-key client.miranda\n")),(0,r.kt)("p",null,"\ud83d\udd35 Client \u2736 Mount (Ceph-fuse)"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    \u203c\ufe0f ceph-fuse is like cephadm. no ububtu22 now. use ubuntu 20! \u203c\ufe0f \n    \u203c\ufe0f ceph-fuse is like cephadm. no ububtu22 now. use ubuntu 20! \u203c\ufe0f \n    \u203c\ufe0f ceph-fuse is like cephadm. no ububtu22 now. use ubuntu 20! \u203c\ufe0f \n\n\n\ud83d\udd36 Insatll ceph-fuse \n\n    \u25ce Ubuntu 20:   sudo apt install ceph-fuse  \n\n\ud83d\udd36 Make dir \n\n    sudo mkdir -p /etc/ceph\n\n\n\ud83d\udd36 Copy Moniter ceph.conf  to Client \n\n    sudo scp {user}@{server-machine}:/etc/ceph/ceph.conf /etc/ceph/ceph.conf\n    sudo scp root@10.12.12.70:/etc/ceph/ceph.conf /etc/ceph/ceph.conf\n\n        monitor node need enable root user & allow root ssh \n\n\n\ud83d\udd36 Copy Moniter xxxx.keyring  to Client\n\n    sudo scp {user}@{server-machine}:/etc/ceph/ceph.keyring /etc/ceph/ceph.keyring\n    sudo scp root@10.12.12.70:/etc/ceph/ceph.client.miranda.keyring /etc/ceph/fs-miranda.keyring\n        \n        this download keyring  and renamed to fs-miranda.keyring \n\n\n\ud83d\udd36 Make mount Folder \n\n    sudo mkdir /mnt/ceph-cephfs-MountPoint\n\n\n\ud83d\udd36 Mount cephfs \n\n    ceph-fuse [-n client.username] [ -m monaddr:port ] mountpoint [ fuse options ]\n    sudo ceph-fuse -n client.miranda -m 10.12.12.70:6789 /mnt/ceph-cephfs-MountPoint\n\n\n\ud83d\udd36 check nfs \n\n    mount -l | grep ceph\n")),(0,r.kt)("p",null,"\ud83d\udd35 Client \u2736 Auto Mount - ceph fs  \ud83d\udcaf"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"\ud83d\udd36 Create Script \n\n    sudo bash -c 'echo \"\n    #!/bin/bash\n    ceph-fuse -n client.miranda -m 10.12.12.70:6789 /mnt/ceph-cephfs-MountPoint\n    \" > /root/crontab-script-ceph-fs-Mount-NoDEL.sh'\n\n\n\ud83d\udd36 Give Perssmion\n\n        chmod +x /root/crontab-script-ceph-fs-Mount-NoDEL.sh\n\n\n\n\ud83d\udd36 crontab desc \n\n    \u29bf  crontab -l   check  task\n    \u29bf  crontab -e   edit/add task\n    \u29bf  crontab -r   remove  task \n\n\n\ud83d\udd36 enter crontab edit \n\n    crontab -e\n\n\n\ud83d\udd36 add a line & save file \n\n    @reboot /root/crontab-script-ceph-fs-Mount-NoDEL.sh\n\n\n\ud83d\udd36 reboot tast \n")),(0,r.kt)("p",null,"===============================================\n\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35Block Device Demo\n\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35Block Device Demo\n\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35Block Device Demo"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://documentation.suse.com/zh-cn/ses/7/html/ses-all/cha-ceph-as-iscsi.html"},"https://documentation.suse.com/zh-cn/ses/7/html/ses-all/cha-ceph-as-iscsi.html"),"\n??"),(0,r.kt)("p",null,"\ud83d\udd35 Server \u2736 Create Block Device Pool"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"\ud83d\udd36 create pool \n\n    ceph osd pool create CEPH_BD-K8s-Prod 128 128 \n    ceph osd pool create CEPH_BD-K8s-Test 128 128 \n\n\n\n\ud83d\udd36 Change repeat size to 2\n\n    ceph osd pool set CEPH_BD-K8s-Prod size 2\n    ceph osd pool set CEPH_BD-K8s-Test size 2\n\n\n\n\ud83d\udd36 Init Pool \n\n    sudo rbd pool init CEPH_BD-K8s-Prod\n    sudo rbd pool init CEPH_BD-K8s-Test \n\n        Filesystem and Object Gateway  No need Initialize \n        only Block Device need use rbd to initialize  \n\n\n\ud83d\udd36 list pool \n\n    ceph osd lspools\n")),(0,r.kt)("p",null,"======================================"),(0,r.kt)("p",null,"\ud83d\udd35 Server \u2736 Create User for Mount "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"\u203c\ufe0f By default Block Device Storage Use admin account to mount, No Need Create User. \u203c\ufe0f\n\u203c\ufe0f By default Block Device Storage Use admin account to mount, No Need Create User. \u203c\ufe0f\n\u203c\ufe0f By default Block Device Storage Use admin account to mount, No Need Create User. \u203c\ufe0f\n\n\n\ud83d\udd36 options \n\n    ceph auth get-or-create client.BD-User01 mon 'profile rbd' osd 'profile {profile name} [pool={pool-name}][, profile ...]' mgr 'profile rbd [pool={pool-name}]'\n\n    \u25ce read only user \n    ceph auth get-or-create client.BD-K8s-Prod-User01 mon 'profile rbd' osd 'profile rbd pool=CEPH_BD-K8s-Prod, profile rbd-read-only pool=images' mgr 'profile rbd pool=images'\n\n    [client.BD-K8s-Prod-User01]\n        key = AQCWq6FiJSG7Ah\n\n    ceph auth get-or-create client.BD-K8s-Test-User01 mon 'profile rbd' osd 'profile rbd pool=CEPH_BD-K8s-Test, profile rbd-read-only pool=images' mgr 'profile rbd pool=images'\n\n    [client.BD-K8s-Test-User01]\n        key = AQCpq6Fiw/sJNBAAQx\n")),(0,r.kt)("p",null,"========================================"),(0,r.kt)("p",null,"\ud83d\udd35 Server \u2736 Create Image \u2705"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"\ud83d\udd36 Image Desc \n\n    Pool is for ceph cluster;   Image is for Client to Mount;\n        Image is like Hardware Disk let client mount & use.\n            Create Image for pool_xxx  first.\n                Create User for Mount.\n                    Client Mount.\n\n    \n    image is created under/inside pool. \n\n\n\ud83d\udd36 Create Image \n\n    rbd create --size {megabytes} {pool-name}/{image-name}\n\n    rbd create --size 10240 CEPH_BD-K8s-Prod/IMG-K8s-Prod\n    rbd create --size 10240 CEPH_BD-K8s-Test/IMG-K8s-Test\n\n        image is under pool.\n        create 10G image under pool \n\n\n\ud83d\udd36 List Image \n\n    rbd ls {poolname}\n    rbd ls CEPH_BD-K8s-Prod\n    rbd ls CEPH_BD-K8s-Test\n\n\n\ud83d\udd36 Check Image info \n\n    rbd info {pool-name}/{image-name}\n    rbd info CEPH_BD-K8s-Prod/IMG-K8s-Prod\n    rbd info CEPH_BD-K8s-Test/IMG-K8s-Test\n\n\n\n\ud83d\udd36 Resize Image  \n\n    rbd resize --size 20480 IMG-K8s-Prod                     \u279c increase size \n    rbd resize --size 1024 IMG-K8s-Test --allow-shrink       \u279c decrease size  \n\n        image only take space when use it. \n            if only create, no put data in. the real size is zero.\n")),(0,r.kt)("p",null,"========================================"),(0,r.kt)("p",null,"\ud83d\udd35 Client \u2736 Mapping Image  \u2705"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"\ud83d\udd36 install Ceph Common\n\n    U20-TempleOnly ~ sudo apt install ceph-common\n\n\n\ud83d\udd36 Copying Conf and Keyring from moniter node\n\n    sudo scp root@10.12.12.70:/etc/ceph/ceph.conf /etc/ceph/ceph.conf\n    sudo scp root@10.12.12.70:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring\n\n        No Change filename on client side after copy\n        when mapping. it use default name to find server info.\n        or next mapping will wrong \n\n\n\ud83d\udd36 mapping\n\n    mapping is like put new disk to client server.\n    if need use disk. still need format & mount.\n\n    \u25ce Check status    rbd showmapped \n    \u25ce map             rbd map Pool_BD-K8s-Prod/IMG-K8s-Prod -p rbd\n")),(0,r.kt)("p",null,"====================="),(0,r.kt)("p",null,"\ud83d\udd35 Client \u2736 Mount disk    \u2705"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"atter Mapping successful.\n    it is like a new hardware disk.\n        format it to xfs &&  mount it; \n            then we can use it.\n\n\n\ud83d\udd36 check disk name \n\n    fdisk -l  \n    Disk /dev/rbd0: 10 GiB, 10737418240 bytes, 20971520 sectors\n\n\n\ud83d\udd36 Format To XFS \n\n    mkfs.xfs /dev/rbd0 \n\n\n\n\ud83d\udd36 Make Mount Point \n\n    mkdir /mnt/ceph-img-k8s-prod\n\n\n\ud83d\udd36 Mount \n\n    mount /dev/rbd0 /mnt/ceph-img-k8s-prod/\n\n\n\ud83d\udd36 Verify \n\n    df -T \n    df -hl | grep rbd\n")),(0,r.kt)("p",null,"\ud83d\udd35 Client \u2736 Auto Mount \ud83d\udcaf"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"\ud83d\udd36 Create Script \n\n    sudo bash -c 'echo \"\n    #!/bin/bash\n    rbd map Pool_BD-K8s_Prod/IMG-K8s-Prod -p rbd\n    mount /dev/rbd0 /mnt/ceph-img-k8s-prod/\n    \" > /root/crontab-script-ceph-DB-Mount-NoDEL.sh'\n\n\n    \ud83d\udd36 Give Perssmion \u203c\ufe0f\n\n        chmod +x /root/crontab-script-ceph-DB-Mount-NoDEL.sh\n\n\n\n\ud83d\udd36 crontab desc \n\n    \u29bf  crontab -l   check  task\n    \u29bf  crontab -e   edit/add task\n    \u29bf  crontab -r   remove  task \n\n\n\ud83d\udd36 enter crontab edit \n\n    crontab -e\n\n\n\ud83d\udd36 add a line & save file \n\n    @reboot /root/crontab-script-ceph-DB-Mount-NoDEL.sh\n\n\n\ud83d\udd36 reboot tast \n")),(0,r.kt)("p",null,"\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\n\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35  OB Demo",(0,r.kt)("br",{parentName:"p"}),"\n","\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\n\u274c\u274c\u274c Give Up.  can not enable fuck serverice.."),(0,r.kt)("p",null,"\u203c\ufe0f Offical manual .."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://docs.ceph.com/en/pacific/radosgw/pools/"},"https://docs.ceph.com/en/pacific/radosgw/pools/")),(0,r.kt)("p",null,"radosgw  commond\n",(0,r.kt)("a",{parentName:"p",href:"https://www.mankier.com/8/radosgw-admin"},"https://www.mankier.com/8/radosgw-admin")),(0,r.kt)("p",null,"\ud83d\udd35 AWS S3 Desc  "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Amazon Simple Storage Service\n\noffer a web.  in web you can control all.\nevery file have a url. \nyou can use it in your web. \ngood for website!!! \n")),(0,r.kt)("p",null,"\ud83d\udd35 Basic info "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"rgw-us-east:\nrealm: replicated\nzonegroup: us\nzone: us-east\nrgw-us-west:\nrealm: replicated\nzonegroup: us\nzone: us-west\n\n\n\nzone + zone ... >> Zone Group \n    zone group + zone group + ... >> realm \n\nall zone under the same zone group  can sync between zone.\n    this make backup between two physical place possible.\n        like Zone-Group-Global\n            zone_1 put in china \n            zone_2 put in Us\n            ...\n\n\n\n\u203c\ufe0f no commit no take effeci:             radosgw-admin period update --commit\n\u203c\ufe0f no commit no take effeci:             radosgw-admin period update --commit\n\u203c\ufe0f no commit no take effeci:             radosgw-admin period update --commit\n")),(0,r.kt)("p",null,"\ud83d\udd36 Add placement target...."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"$ radosgw-admin zonegroup placement add \\\n    --rgw-zonegroup OB-Group-Ark \\\n    --placement-id temporary\n\n\n$ radosgw-admin zone placement add \\\n    --rgw-zone OB-Zone-Ark \\\n    --placement-id temporary \\\n    --data-pool default.rgw.temporary.data \\\n    --index-pool default.rgw.temporary.index \\\n    --data-extra-pool default.rgw.temporary.non-ec\n")),(0,r.kt)("p",null,"\ud83d\udd35 "),(0,r.kt)("p",null,"\ud83d\udd36 Create realm:   "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"radosgw-admin realm create --rgw-realm=OB-Realm-Ark --default \n")),(0,r.kt)("p",null,"\ud83d\udd36 Create zone group:  "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"radosgw-admin zonegroup create --rgw-zonegroup=OB-Group-Ark --master --default\n")),(0,r.kt)("p",null,"\ud83d\udd36 Create zone:  "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"radosgw-admin zone create --rgw-zonegroup=OB-Group-Ark --rgw-zone=OB-Zone-Ark --master --default\n")),(0,r.kt)("p",null,"\ud83d\udd36 update period"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"radosgw-admin period update --rgw-realm=OB-Realm-Ark --commit \n")),(0,r.kt)("p",null,"\ud83d\udd36 Create RGW  "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'orch apply rgw <realm_name> <zone_name> \n    ceph orch apply rgw OB-Realm-Ark OB-Zone-Ark --placement="1 CEPH-Node"\n')),(0,r.kt)("p",null,"\ud83d\udd36 Check rgw "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"ceph orch ps --daemon-type rgw\nNo daemons reported\n\n\ud83d\udc1e web:   No RGW service is running.\n")),(0,r.kt)("p",null,"\ud83d\udd36 Create USER  \u274c"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'radosgw-admin user create --uid=rgw-user --display-name=rgw-user --system\n\n\n"user_id": "rgw-user",\n"display_name": "rgw-user",\n\n        "user": "rgw-user",\n        "access_key": "XD0QFJQCEJMSLEYQACGS",\n        "secret_key": "WboesDxdzMezAMwQd85xpL26BAckiJfCvlZpt0Ci"\n\n\n\n\u8bb0\u5f55access_key\u548csecret_key\u7684\u503c\u4fdd\u5b58\u4e3a access_key.txt \u548csecret_key.txt \uff0c\u901a\u8fc7\u547d\u4ee4\u96c6\u6210\u5230dahsboard\u4e2d\u3002\n\nceph dashboard set-rgw-api-access-key -i accesskey.txt \nceph dashboard set-rgw-api-secret-key -i secretkey.txt \n')),(0,r.kt)("p",null,"\ud83d\udd35 OB  client .."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'# \u5b89\u88c5 AWS s3 API\n[root@ceph01 ~]# yum -y install s3cmd\n\n# \u521b\u5efa\u7528\u6237\n[root@ceph01 ~]# radosgw-admin user create --uid=s3 --display-name="objcet storage" --system\n\n# \u83b7\u53d6\u7528\u6237 access_key \u548c secret_key\n[root@ceph01 ~]# radosgw-admin user info --uid=s3 | grep -E "access_key|secret_key"\n            "access_key": "RPRUFOWDK0x",\n            "secret_key": "32efWJ7OKx"\n\n# \u751f\u6210 S3 \u5ba2\u6237\u7aef\u914d\u7f6e(\u8bbe\u7f6e\u4e00\u4e0b\u53c2\u6570, \u5176\u4f59\u9ed8\u8ba4\u5373\u53ef)\n[root@ceph01 ~]# s3cmd --configure\nAccess Key: RPRUFOx\nSecret Key: 32efWJ7OeKJx\nS3 Endpoint [s3.amazonaws.com]: ceph01\nDNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket).ceph01\nUse HTTPS protocol [Yes]: no\nTest access with supplied credentials? [Y/n] y\nSave settings? [y/N] y\nConfiguration saved to \'/root/.s3cfg\'\n\n# \u521b\u5efa\u6876\n[root@ceph01 ~]# s3cmd mb s3://bucket\nBucket \'s3://bucket/\' created\n\n# \u67e5\u770b\u5f53\u524d\u6240\u6709\u6876\n[root@ceph01 ~]# s3cmd ls\n2020-06-28 03:02  s3://bucket\n-----------------------------------\n\xa9\u8457\u4f5c\u6743\u5f52\u4f5c\u8005\u6240\u6709\uff1a\u6765\u81ea51CTO\u535a\u5ba2\u4f5c\u8005\u7ea2\u5c18\u4e16\u95f4\u7684\u539f\u521b\u4f5c\u54c1\uff0c\u8bf7\u8054\u7cfb\u4f5c\u8005\u83b7\u53d6\u8f6c\u8f7d\u6388\u6743\uff0c\u5426\u5219\u5c06\u8ffd\u7a76\u6cd5\u5f8b\u8d23\u4efb\n\u4f7f\u7528cephadm\u5feb\u901f\u642d\u5efaceph\u96c6\u7fa4\nhttps://blog.51cto.com/hongchen99/2507660\n')),(0,r.kt)("p",null,"\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\n\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\n\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35 ceph auth / user permit "),(0,r.kt)("p",null,"\ud83d\udd35 WHY CephX "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"both ceph(server) and k8s(client) have a same key.\nuse this can avoid hacker atteck. \n\nif any client need use ceph.\nceph have to create use first.\nthen ceph will create a username & a key.\n\n  [client.k8s]\n      key = AQBTvaZi0HhkABAAgn \n")),(0,r.kt)("p",null,"\ud83d\udd35 CephX Auth "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"CephX manager whole ceph cluster login system.\n\ninside ceph:  \n    not only user(clietn.admin .. ) need password. \n    mon/osd/mds need password to login ceph too.\n\n\noutside ceph: (like k8s cluster) \n    if k8s want connect/use ceph cluster must need four info.\n        \u25ce ceph cluster id:   fsid \n        \u25ce ceph cluster monitor ip address. \n        \u25ce username \n        \u25ce password \n\n\nwe use mon node to build ceph cluster.\nso even you lost client.admin password. \nmon node can reset password for admin.\n")),(0,r.kt)("p",null,"\ud83d\udd35 permit analyze  "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"CEPH-MGR.Root ~ ceph auth ls\n\nclient.admin\n    key: AQC08qBirX7TLhAAn\n    caps: [mds] allow *\n    caps: [mgr] allow *\n    caps: [mon] allow *\n    caps: [osd] allow *\n\nCEPH-MGR.Root ~ ceph auth ls\nclient.k8s\n    key: AQBTvaZi0HhkABAAgYs \n    caps: [mgr] profile rbd pool=Pool_BD-K8s_Test\n    caps: [mon] profile rbd\n    caps: [osd] profile rbd pool=Pool_BD-K8s_Test\n\nclient.miranda\n    key: AQC8nKFiQgA+A\n    caps: [mds] allow r, allow rw path=/\n    caps: [mon] allow r\n    caps: [osd] allow rw pool=data\n\n\n\n\ud83d\udd36 permit \n\n    *  = rwx\n    r:   allow read \n    w:   allow write \n    x:   allow run auth commonds (ceph auth list / ceph guth get)\n\n\n\ud83d\udd36 [mon] / [mgr] / [mds]  / [osd]\n\n    \u25ce caps:  capabilities \n\n    ceph cluster have a lot role.\n    differnet role have differnet job/function.\n    if you need give someone ceph`s permission.\n    need add role permit first.  then set detail permit under that role. \n\n        caps: [osd] allow *               \u279c have all control to osd role.\n        caps: [osd] allow rw pool=data    \u279c only read+wirte under data pool. \n        \n\n\ud83d\udd36 profile rbd pool=Pool_BD-K8s_Test\n")),(0,r.kt)("p",null,"\ud83d\udd35 Set Permit Demo "),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://www.shuzhiduo.com/A/mo5k4alQdw/"},"https://www.shuzhiduo.com/A/mo5k4alQdw/")),(0,r.kt)("p",null,"\ud83d\udd35 Create/get User "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"ceph auth get-or-create client.k8s mon 'allow r' osd 'allow rw pool=Pool_BD-K8s_Test'\n")),(0,r.kt)("p",null,"\ud83d\udd35 Change User Permit \u2705"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"ceph auth get client.k8s\n\nceph auth caps client.k8s mon 'allow r' osd 'allow rw pool=Pool_BD-K8s_Test'    \u279c rw  one pool\nceph auth caps client.k8s mon 'allow r' osd 'allow *'                           \u279c rw  all pool\n")))}d.isMDXComponent=!0}}]);