"use strict";(self.webpackChunkblog_never_del=self.webpackChunkblog_never_del||[]).push([[9833],{3905:(n,e,t)=>{t.d(e,{Zo:()=>l,kt:()=>m});var o=t(7294);function s(n,e,t){return e in n?Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):n[e]=t,n}function r(n,e){var t=Object.keys(n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(n);e&&(o=o.filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable}))),t.push.apply(t,o)}return t}function a(n){for(var e=1;e<arguments.length;e++){var t=null!=arguments[e]?arguments[e]:{};e%2?r(Object(t),!0).forEach((function(e){s(n,e,t[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(t,e))}))}return n}function i(n,e){if(null==n)return{};var t,o,s=function(n,e){if(null==n)return{};var t,o,s={},r=Object.keys(n);for(o=0;o<r.length;o++)t=r[o],e.indexOf(t)>=0||(s[t]=n[t]);return s}(n,e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(n);for(o=0;o<r.length;o++)t=r[o],e.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(n,t)&&(s[t]=n[t])}return s}var d=o.createContext({}),c=function(n){var e=o.useContext(d),t=e;return n&&(t="function"==typeof n?n(e):a(a({},e),n)),t},l=function(n){var e=c(n.components);return o.createElement(d.Provider,{value:e},n.children)},u={inlineCode:"code",wrapper:function(n){var e=n.children;return o.createElement(o.Fragment,{},e)}},p=o.forwardRef((function(n,e){var t=n.components,s=n.mdxType,r=n.originalType,d=n.parentName,l=i(n,["components","mdxType","originalType","parentName"]),p=c(t),m=s,h=p["".concat(d,".").concat(m)]||p[m]||u[m]||r;return t?o.createElement(h,a(a({ref:e},l),{},{components:t})):o.createElement(h,a({ref:e},l))}));function m(n,e){var t=arguments,s=e&&e.mdxType;if("string"==typeof n||s){var r=t.length,a=new Array(r);a[0]=p;var i={};for(var d in e)hasOwnProperty.call(e,d)&&(i[d]=e[d]);i.originalType=n,i.mdxType="string"==typeof n?n:s,a[1]=i;for(var c=2;c<r;c++)a[c]=t[c];return o.createElement.apply(null,a)}return o.createElement.apply(null,t)}p.displayName="MDXCreateElement"},5359:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>d,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var o=t(7462),s=(t(7294),t(3905));const r={sidebar_position:1930,title:"\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c Build"},a="Storage \u2736 CEPH \u279c Build",i={unversionedId:"\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c Build",id:"\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c Build",title:"\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c Build",description:"\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f",source:"@site/docs/\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c Build.md",sourceDirName:".",slug:"/\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c Build",permalink:"/\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c Build",draft:!1,tags:[],version:"current",sidebarPosition:1930,frontMatter:{sidebar_position:1930,title:"\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c Build"},sidebar:"defaultSidebar",previous:{title:"\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0 S3 \u279c MinIO",permalink:"/\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0 S3 \u279c MinIO"},next:{title:"\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c Use",permalink:"/\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c Use"}},d={},c=[],l={toc:c};function u(n){let{components:e,...t}=n;return(0,s.kt)("wrapper",(0,o.Z)({},l,t,{components:e,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"storage--ceph--build"},"Storage \u2736 CEPH \u279c Build"),(0,s.kt)("p",null,"\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\n",(0,s.kt)("a",{parentName:"p",href:"https://www.ityww.cn/1588.html"},"https://www.ityww.cn/1588.html")),(0,s.kt)("p",null,"\ud83d\udd35 VM Prepair   \u2705"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\ud83d\udd36 K8s Role Desc \n\n    only two kinds Role :  manager and worker(osd) \n        no need manual set/choose monitor node \n            during install. ceph automatic choose/set all the role.\n\n\n\ud83d\udd36 VM System Version \n\n    https://docs.ceph.com/en/quincy/start/os-recommendations/\n\n    \u203c\ufe0f Ceph 17.2 is test on Ubuntu_20 only.  No Ubuntu_22 yet \u203c\ufe0f\n    \u203c\ufe0f Ceph 17.2 is test on Ubuntu_20 only.  No Ubuntu_22 yet \u203c\ufe0f\n    \u203c\ufe0f Ceph 17.2 is test on Ubuntu_20 only.  No Ubuntu_22 yet \u203c\ufe0f\n\n\n\ud83d\udd36 VM Number: must at least two   \n\n    CEPH-Mgr    x 1  \n\n    CEPH-Node   x 1 \n        3 x 345G  Physical iscsi disk from nas \n        1 x 1T    Physical iscsi disk from nas \n\n\n\ud83d\udd36 VM NIC \n\n    \u25ce Manager\n        VLAN-STO-1012-CEPH_internel   10.12.12.0/24     Must   \u279c  ceph cluster internal commute.\n\n    \u25ce Worker\n        VLAN-STO-1010-NAS_ISCSI       10.10.10.0/24     Option \u279c  nas. provide disk to worker\n        VLAN-STO-1012-CEPH_internel   10.12.12.0/24     Must   \u279c  ceph cluster internal commute.\n\n\n\n\ud83d\udd36 VM Basic Setup \n\n    \u25ce Static IP \n    \u25ce update apt Source \n    \u25ce install docker \n")),(0,s.kt)("p",null,"\ud83d\udd35 VM Mount iscsi disk   \u2705"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"https://manjaro.site/how-to-connect-to-iscsi-volume-from-ubuntu-20-04/\n\n\ud83d\udd36 iSCSI Initiator Install \n\n    sudo apt install open-iscsi\n\n\n\ud83d\udd36 Config network card for iscsi \n\n    \u25ce why \n        we have three nic.\n        give nic a nickname  \n        easy for manager.\n\n\n\nnetwork:\nethernets:\n    ens160:\n    match:\n        macaddress: 00:50:56:85:b8:cc\n    set-name: Nic-CEPH_1012\n    dhcp4: false\n    addresses: [10.12.12.77/24]\n    gateway4: 10.12.12.11\n    nameservers:\n        addresses: [10.12.12.11,10.12.12.1]\n    optional: true\n    ens224:\n    match:\n        macaddress: 00:50:56:85:ec:43\n    set-name: Nic-NAS_1010\n    dhcp4: false\n    addresses: [10.10.10.77/24]\n    optional: true\n    ens192:\n    match:\n        macaddress: 00:50:56:85:55:a5\n    set-name: Nic-CEPH_1011\n    dhcp4: false\n    addresses: [10.11.11.77/24]\n    optional: true\nversion: 2\n\n\n\n\n\n\ud83d\udd36 config iscsi auth \n\n    \u25ce if you nas need chap.  do this.    otherwise skip.\n        sudo vi /etc/iscsi/iscsid.conf\n            node.session.auth.authmethod = CHAP\n            node.session.auth.username = username\n            node.session.auth.password = password\n\n\n\ud83d\udd36 enable server on boot \n\n    sudo systemctl enable open-iscsi\n    sudo systemctl enable iscsid\n\n\n\ud83d\udd36 Discover remote nas iscsi target\n\n    CEPH-99 ~ sudo iscsiadm -m discovery -t sendtargets -p 10.10.10.88\n    10.10.10.88:3260,1 iqn.2000-01.com.synology:NAS-DS2015XS.Target-CEPH11.b6f8cfe4bdb\n    10.10.10.88:3260,1 iqn.2000-01.com.synology:NAS-DS2015XS.Target-CEPH22.b6f8cfe4bdb\n    10.10.10.88:3260,1 iqn.2000-01.com.synology:NAS-DS2015XS.Target-CEPH33.b6f8cfe4bdb\n\n\n\ud83d\udd36 manual connect target \n\nsudo iscsiadm --mode node --targetname iqn.2000-01.com.synology:NAS-DS2015XS.Target-CEPH11.b6f8cfe4bdb --portal 10.10.10.88 --login\nsudo iscsiadm --mode node --targetname iqn.2000-01.com.synology:NAS-DS2015XS.Target-CEPH22.b6f8cfe4bdb --portal 10.10.10.88 --login\nsudo iscsiadm --mode node --targetname iqn.2000-01.com.synology:NAS-DS2015XS.Target-CEPH33.b6f8cfe4bdb --portal 10.10.10.88 --login\n\n\n\ud83d\udd36 Comnand \n    \n    \u25ce restart services                 sudo systemctl restart iscsid open-iscsi \n    \u25ce List connected iscsi node        sudo iscsiadm -m node -l\n    \u25ce Disconnect all iscsi node        sudo iscsiadm -m node -u\n\n    \u25ce List Disk                        sudo fdisk -l\n        now you should see your iscsi disk at here.\n        next is auto connect iscsi tatget at boot.\n\n\n\n\n\n\ud83d\udd36 auto mount:  Config iscsi node \n\n    use root user.\n    cd cd /etc/iscsi/nodes\n        config every iscsi taeget you need auto connect.\n    enter iscsi tagert  config default file.\n        change \n        node.startup = manual    to     node.startup = automatic\n            sed -i 's/node.startup = manual/node.startup = automatic/g' ./default\n\n            cat ./default | grep node.startup\n                node.startup = automatic\n\n        reboot server. check result.\n\n\nCEPH-99 ~ sudo su -\nCEPH-99.Root ~ cd /etc/iscsi/nodes\nCEPH-99.Root nodes ls\niqn.2000-01.com.synology:NAS-DS2015XS.Target-0000.b6f8cfe4bdb\niqn.2000-01.com.synology:NAS-DS2015XS.Target-CEPH11.b6f8cfe4bdb\niqn.2000-01.com.synology:NAS-DS2015XS.Target-CEPH22.b6f8cfe4bdb\niqn.2000-01.com.synology:NAS-DS2015XS.Target-CEPH33.b6f8cfe4bdb\nCEPH-99.Root nodes cd iqn.2000-01.com.synology:NAS-DS2015XS.Target-CEPH11.b6f8cfe4bdb\nCEPH-99.Root iqn.2000-01.com.synology:NAS-DS2015XS.Target-CEPH11.b6f8cfe4bdb ls\n10.10.10.88,3260,1\nCEPH-99.Root iqn.2000-01.com.synology:NAS-DS2015XS.Target-CEPH11.b6f8cfe4bdb cd 10.10.10.88,3260,1\nCEPH-99.Root 10.10.10.88,3260,1 ls\ndefault\n")),(0,s.kt)("p",null,"\ud83d\udd35 All Node: NTP & Timezone \u2705"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\ud83d\udd36 NTP \n\n    \u25ce status                  sudo systemctl status ntp\n    \u25ce add local ntp server    sudo vi /etc/ntp.conf \n    \u25ce restart                 sudo service ntp restart\n\n\n\ud83d\udd36 Timezone \n\n    timedatectl list-timezones\n    sudo timedatectl set-timezone America/Los_Angeles\n")),(0,s.kt)("p",null,"\ud83d\udd35 All Node: Hostname & Hosts"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\ud83d\udd36 Change Hostname \n\n    hostnamectl set-hostname  CEPH-Mgr\n    hostnamectl set-hostname  CEPH-Node \n\n    \u203c\ufe0f ceph host name need like  CEPH-Node03   not  CEPH-Node03.ark (fqdn) \u203c\ufe0f\n\n\n\ud83d\udd36 Config Hosts file \n\n    >  /etc/hosts   means replace \n    >> /etc/hosts   means append  \n\n\nsudo bash -c 'echo \"\n127.0.0.1   localhost\n10.12.12.70 CEPH-Mgr\n10.12.12.77 CEPH-Node\" > /etc/hosts'\n")),(0,s.kt)("p",null,"\ud83d\udd35 Mgr Node: install Tool: Cephadm"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\ud83d\udd36 Ubuntu 20\n\n    sudo apt install -y cephadm\n")),(0,s.kt)("p",null,"\ud83d\udd35 Mgr Node: deploy Monitor "),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"    \u203c\ufe0f Moniter must install in local machine which have cephadm installed. here: Manger node \u203c\ufe0f\n        ip choose ceph internel vlan. \n\nsudo cephadm bootstrap --mon-ip 10.12.12.70\n\n        during install . remember password. \n\n     URL: https://CEPH-Mgr:8443/\n    User: admin\nPassword: r3uq4z2agg\n\n\n\ud83d\udd36 what this do\n\n    create ssh key. \n    later you need upload this public key to other node\n\n    and create a lot docker to mgr. \n")),(0,s.kt)("p",null,"\ud83d\udd35 Monitor web login "),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"https://10.12.12.70:8443/\nhttps://10.12.12.70:3000/\n")),(0,s.kt)("p",null,"\ud83d\udd35 Monitor Enable ceph command "),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"sudo cephadm install ceph-common\n\n    if no this.\n        you can not use ceph command under ceph-mgr     \n            you have to enter cephadm shell first\n                then use ceph command under cephadm shell\n                    sudo cephadm shell\n                    ceph -s\n\n        \n")),(0,s.kt)("p",null,"\ud83d\udd35 upload ssh key To other node "),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"cephadm already create key for you . \nno need crtate ssh key yourself .\n\nas for user we use root.\nby default root is disabled in ubuntu \n\n\n\ud83d\udd36 ALL Node: Config root User\n\n    \u25ce Enable root \n        sudo passwd root\n\n    \u25ce Allow Root ssh \n\n        sudo sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/g' /etc/ssh/sshd_config\n        sudo service sshd restart\n\n\n\ud83d\udd36 CEPH-Mgr Upload public key to other node \n\n    ssh-copy-id -f -i /etc/ceph/ceph.pub root@CEPH-Node\n\n\n    \n")),(0,s.kt)("p",null,"\ud83d\udd35 Add node to ceph Cluster. "),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\ud83d\udd36 add \n\n    CEPH-Mgr ~ ceph orch host add CEPH-Node \n\n\n\ud83d\udd36 check host \n\n    CEPH-Mgr ~ sudo ceph orch  host ls\n    HOST       ADDR       LABELS  STATUS\n    CEPH-Mgr   CEPH-Mgr\n    CEPH-Node  CEPH-Node\n")),(0,s.kt)("p",null,"\ud83d\udd35 Auto Deploy Monitor "),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"chepadm will automatic deploy manager & monitor node to some host under cluster.\nceph need many manager & monitor for high ability.\njust wait. you need do nothing here. \nno need manual choose which host use what service (manager / monitor)\n\n\nCEPH-Mgr ~ sudo ceph -s\ncluster:\n    id:     b57b2062-e75d-11ec-8f3e-45be4942c0cb\n    health: HEALTH_WARN\n            OSD count 0 < osd_pool_default_size 3\n\nservices:\n    mon: 1 daemons, quorum CEPH-Mgr (age 17m)\n    mgr: CEPH-Mgr.ljhrsr(active, since 16m), standbys: CEPH-Node.lemojs\n    osd: 0 osds: 0 up, 0 in\n")),(0,s.kt)("p",null,"\ud83d\udd35 OSD  setup ."),(0,s.kt)("p",null,"\ud83d\udd36 All Node Check available disk "),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"i only use ceph-node mount all physical disk.\n\nCEPH-Node ~ lsblk\n    sdb                         8:16   0  345G  0 disk\n    sdc                         8:32   0  345G  0 disk\n    sdd                         8:48   0  345G  0 disk\n")),(0,s.kt)("p",null,"\ud83d\udd36 add disk to ceph cluster "),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"in ceph-mgr \n\nsudo ceph orch daemon add osd CEPH-Node:/dev/sdb \nsudo ceph orch daemon add osd CEPH-Node:/dev/sdc\nsudo ceph orch daemon add osd CEPH-Node:/dev/sdd\n\n\n    if disk not empty. need use blow command first\n     sgdisk --zap-all /dev/sdxxxx\n")))}u.isMDXComponent=!0}}]);