"use strict";(self.webpackChunkblog_never_del=self.webpackChunkblog_never_del||[]).push([[3123],{3905:(e,n,t)=>{t.d(n,{Zo:()=>i,kt:()=>k});var r=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function l(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?l(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):l(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function p(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},l=Object.keys(e);for(r=0;r<l.length;r++)t=l[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(r=0;r<l.length;r++)t=l[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var c=r.createContext({}),s=function(e){var n=r.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},i=function(e){var n=s(e.components);return r.createElement(c.Provider,{value:n},e.children)},u={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},d=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,l=e.originalType,c=e.parentName,i=p(e,["components","mdxType","originalType","parentName"]),d=s(t),k=a,m=d["".concat(c,".").concat(k)]||d[k]||u[k]||l;return t?r.createElement(m,o(o({ref:n},i),{},{components:t})):r.createElement(m,o({ref:n},i))}));function k(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var l=t.length,o=new Array(l);o[0]=d;var p={};for(var c in n)hasOwnProperty.call(n,c)&&(p[c]=n[c]);p.originalType=e,p.mdxType="string"==typeof e?e:a,o[1]=p;for(var s=2;s<l;s++)o[s]=t[s];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"},2111:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>l,metadata:()=>p,toc:()=>s});var r=t(7462),a=(t(7294),t(3905));const l={sidebar_position:1930,title:"\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c\u279c Driver"},o="K8s \u2736 Install Ceph-CSI Driver",p={unversionedId:"\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c\u279c Driver",id:"\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c\u279c Driver",title:"\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c\u279c Driver",description:"\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f",source:"@site/docs/\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c\u279c Driver.md",sourceDirName:".",slug:"/\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c\u279c Driver",permalink:"/\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c\u279c Driver",draft:!1,tags:[],version:"current",sidebarPosition:1930,frontMatter:{sidebar_position:1930,title:"\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c\u279c Driver"},sidebar:"defaultSidebar",previous:{title:"\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c Use",permalink:"/\ud83c\udfaa-9\ufe0f\u20e3 STO \ud83d\udcc0\ud83d\udcc0\ud83d\udcc0 CEPH \u279c\u279c Use"},next:{title:"\ud83c\udfaa\ud83c\udfaa\ud83d\udc2c Docker \u279c Basic",permalink:"/\ud83c\udfaa\ud83c\udfaa \ud83d\udc2c Docker \u279c Basic"}},c={},s=[],i={toc:s};function u(e){let{components:n,...t}=e;return(0,a.kt)("wrapper",(0,r.Z)({},i,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"k8s--install-ceph-csi-driver"},"K8s \u2736 Install Ceph-CSI Driver"),(0,a.kt)("p",null,"\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\n",(0,a.kt)("a",{parentName:"p",href:"https://daimajiaoliu.com/daima/8c4744d11e93007"},"https://daimajiaoliu.com/daima/8c4744d11e93007")),(0,a.kt)("p",null,"\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35 CEPH-CSI Integrating K8s "),(0,a.kt)("p",null,"  \ud83d\udd35 WHY\nafter you have build your own ceph cluster.\nnext is let app: docler/k8s/esxi/vm  use ceph disk.\nceph-rbd is most useful."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"  you have two option make app use ceph-rbd.\n  1. manual mount ceph-rdb disk to app (like docker) \n      use just like local disk. \n\n  2. no manyal any ceph-rdb disk to app (like K8s)\n      install a ceph-csi  driver to app.\n          let app can control ceph cluster directly.\n")),(0,a.kt)("p",null,"\ud83d\udd35 POD \u2736 ConfigMap Desc   \u2705"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"configmap allow you sperate config file and image file.\nit make conatin much moveable.\n\nlike your database`d password/token.\nnever write it into container \nso create a configmap. put password in configmap.\n")),(0,a.kt)("p",null,"\ud83d\udd35 Ceph Why & How   \u2705"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"\ud83d\udd36 WHY  \n\n    Container `s data  is tempelory.\n    k8s can create as many contain as you like.\n    but when delete contain. the data in contain is lost too.\n\n\n    CEPH as storage . we need use it.\n    not only simple mount ceph disk(rbd/image) to vm/esxi \n    we can go deeper. let k8s cluster use ceph \n        combine ceph-rbd & StorageClass \n            let ceph give storage to pod. \n\n\n\ud83d\udd36 How\n\n    there is lots ways to use ceph.\n    for now maybe rbd-provisioner is best \n")),(0,a.kt)("p",null,"\ud83d\udd35 K8s Storage flow "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"Pod >> PVC  >> PV  >> Storage \n\nPhysical Storage: NFS/ISCSI/CEPH-RBD.....\n    >> PV: (Provide Storage): \n        >> PVC (Use Storage): \n            >> Namaspace \n                >> Pod (Volume)\n\n    Docker User:      Create PVC \n    IT Admin:         Create PV.\n")),(0,a.kt)("p",null,"\ud83d\udd35 Clean Old file - option "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"if you tried install ceph-csi before. \n\u203c\ufe0f if possibel  reset k8s cluster.  \u203c\ufe0f\n\u203c\ufe0f if possibel  reset k8s cluster.  \u203c\ufe0f\n\u203c\ufe0f if possibel  reset k8s cluster.  \u203c\ufe0f\n")),(0,a.kt)("p",null,"  \ud83d\udd36 configmap"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"  kubectl get configmap\n  kubectl delete configmap xxx\n")),(0,a.kt)("p",null,"  \ud83d\udd36 services"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"  kubectl get services\n  kubectl delete services xxxx\n")),(0,a.kt)("p",null,"  \ud83d\udd36 deployments.apps"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"  kubectl get deployments.apps\n  kubectl delete deployments.apps csi-rbdplugin-provisioner\n")),(0,a.kt)("p",null,"\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\n\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35 ceph-csi\n\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35"),(0,a.kt)("p",null,"\ud83d\udd35 CEPH-CSI "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"https://docs.ceph.com/en/latest/rbd/rbd-kubernetes/?highlight=CSI\n\nhttps://github.com/ceph/ceph-csi\n")),(0,a.kt)("p",null,"\ud83d\udd36 CEPH-CSI offical Config Manual "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"\u203c\ufe0f  https://docs.ceph.com/en/latest/rbd/rbd-kubernetes/?highlight=CSI\n\u203c\ufe0f  https://docs.ceph.com/en/latest/rbd/rbd-kubernetes/?highlight=CSI\n\u203c\ufe0f  https://docs.ceph.com/en/latest/rbd/rbd-kubernetes/?highlight=CSI\n")),(0,a.kt)("p",null,"==\n\ud83d\udd35 CEPH Prepair \u279c Pool & User \u2705"),(0,a.kt)("p",null,"\ud83d\udd36 Create Pool / image "),(0,a.kt)("p",null,"  CEPH-MGR.Root ~ ceph osd lspools\nPool_BD-K8s_Test\nCEPH-MGR.Root ~ rbd ls Pool_BD-K8s_Test\nIMG-K8s-Test"),(0,a.kt)("p",null,"\ud83d\udd36 Disable some img function "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"rbd feature disable POOL_NAME/IMAGE_NAME FEATURE_NAME\n\n  rbd feature disable Pool_BD-K8s_Test/IMG-K8s-Test deep-flatten\n  rbd feature disable Pool_BD-K8s_Test/IMG-K8s-Test fast-diff\n  rbd feature disable Pool_BD-K8s_Test/IMG-K8s-Test object-map\n  rbd feature disable Pool_BD-K8s_Test/IMG-K8s-Test exclusive-lock\n")),(0,a.kt)("p",null,"\ud83d\udd36 Create User & Key:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"ceph auth get-or-create client.k8s mon 'allow r' osd 'allow rw pool=Pool_BD-K8s_Test'\n\n  [client.k8s]\n      key = AQBTvaZi0HhkABAAgYa1I5 \n        \u25ce \u279c  \u203c\ufe0fname:  k8s \n        \u25ce \u279c  \u203c\ufe0fkey:   AQBTvaZi0HhkABA \n")),(0,a.kt)("p",null,"\ud83d\udd36 Check User permit"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'CEPH-MGR.Root ~ ceph auth get client.k8s\nexported keyring for client.k8s\n[client.k8s]\n  key = AQBTvaZi0HhkABAAgYa1 \n  caps mon = "allow r"\n  caps osd = "allow rw pool=Pool_BD-K8s_Test"\n')),(0,a.kt)("p",null,"\ud83d\udd36 User Option (use admin)"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"CEPH-MGR.Root ~ ceph auth get client.admin\nexported keyring for client.admin\n[client.admin]\n  key = AQC08qBirX7T \n")),(0,a.kt)("p",null,"\ud83d\udd36 Check cluster id & Monitor IP"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"CEPH-MGR.Root ~ ceph mon dump\nfsid b57b2062-e75d-11ec-8f3e-45be4942c0cb       \u279c \u203c\ufe0f  need this \n0: [v2:10.12.12.70:3300/0,v1:10.12.12.70:6789/0] mon.CEPH-Mgr  \u203c\ufe0f \u279c need v1 ip & port \n    two version:  v1 v2.\n    ceph-csi only support v1. for now. \n")),(0,a.kt)("p",null,"==\n\ud83d\udd35 K8s  network Prepair \u2705"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"ALL K8s node.\n")),(0,a.kt)("p",null,"  \ud83d\udd36 Install Package "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"  sudo apt install -y cephadm\n      cephadm package include ceph-common. \n        you can try only install ceph-common -.-\n      use ubuntu_20 will save you lot time.\n")),(0,a.kt)("p",null,"  \ud83d\udd36 Network Config  \u2705"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"\u203c\ufe0f all k8s need connect to ceph manager 6789 port)\u203c\ufe0f\n  give same portgroup nic to k8s.\n    \n    \u25ce Test 6789 port connection \n    nmap -p 6789 10.12.12.70\n")),(0,a.kt)("p",null,"==\n\ud83d\udd35 Workflow \u203c\ufe0f"),(0,a.kt)("p",null,"  \ud83d\udd36 Tips "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"  1. Not create new namespace for ceph-csi  \n      use default: default \n      if changed namespace. need replace at a lot place in other yaml.\n\n  2. No change any ceph-csi yaml filename we download next move.\n      file name is configed in other yaml! \n")),(0,a.kt)("ol",{start:0},(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"download github ceph-csi.\n\u25ce include all yaml we need, just edit some yaml,  apply.  and done! ")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"set Ceph Cluster info\n\u25ce k8s need konw ceph cluster ip to connect.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"set Ceph Username & password\n\u25ce k8s need ceph`s permission (username & token) to control ceph-rbd disk.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"copy&paste  no change")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"copy&paste  no change")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"copy&paste  no change")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"check result"))),(0,a.kt)("p",null,"\ud83d\udd35 0. Download github Ceph-CSI      \u2705"),(0,a.kt)("p",null,"  \ud83d\udd36 download "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"git clone --depth 1 --branch devel https://github.com/ceph/ceph-csi.git\n")),(0,a.kt)("p",null,"  \ud83d\udd36 enter "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"K8s-Manager kubernetes pwd\n/root/ceph-csi/deploy/rbd/kubernetes\nK8s-Manager kubernetes ll\ntotal 40K\n-rw-r--r-- 1 root root  309 Jun 15 19:30 csi-config-map.yaml\n-rw-r--r-- 1 root root  435 Jun 15 19:30 csidriver.yaml\n-rw-r--r-- 1 root root 1.8K Jun 15 19:30 csi-nodeplugin-psp.yaml\n-rw-r--r-- 1 root root 1.1K Jun 15 19:30 csi-nodeplugin-rbac.yaml\n-rw-r--r-- 1 root root 1.2K Jun 15 19:30 csi-provisioner-psp.yaml\n-rw-r--r-- 1 root root 3.2K Jun 15 19:30 csi-provisioner-rbac.yaml\n-rw-r--r-- 1 root root 8.0K Jun 15 19:30 csi-rbdplugin-provisioner.yaml\n-rw-r--r-- 1 root root 7.1K Jun 15 19:30 csi-rbdplugin.yaml\n")),(0,a.kt)("p",null,"\ud83d\udd35 1. csi-config-map.yaml        \u279c  set Ceph Cluster info         \u2705"),(0,a.kt)("p",null,"  \u203c\ufe0f enter /root/ceph-csi/deploy/rbd/kubernetes;  edit file only. no need create file! \u203c\ufe0f\n\u203c\ufe0f enter /root/ceph-csi/deploy/rbd/kubernetes;  edit file only. no need create file! \u203c\ufe0f"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'cat <<EOF > csi-config-map.yaml\n---\napiVersion: v1\nkind: ConfigMap\ndata:\n  config.json: |-\n    [\n      {\n        "clusterID": "b57b2062-e75d-11ec-8f3e-45be4942c0cb",   # \u203c\ufe0f ChanegMe-01 \n        "monitors": [\n          "10.12.12.70:6789"                                   # \u203c\ufe0f ChangeMe-02\n        ]\n      }\n    ]\nmetadata:\n  name: ceph-csi-config\nEOF\n\n\n\ud83d\udd36 apply yaml       \u279c    kubectl apply -f csi-config-map.yaml\n\ud83d\udd36 check configmap  \u279c    kubectl get configmap\n\nkubectl delete configmap ceph-csi-config\n')),(0,a.kt)("p",null,"==\n\ud83d\udd35 2. csi-rbd-secret.yaml        \u279c  set Ceph Username & password  \u2705"),(0,a.kt)("p",null,"  \u203c\ufe0f try use ceph admin if possible .  other user have unknow problem\n\u203c\ufe0f try use ceph admin if possible .  other user have unknow problem\n\u203c\ufe0f try use ceph admin if possible .  other user have unknow problem "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"  cat <<EOF > csi-rbd-secret.yaml\n  ---\n  apiVersion: v1\n  kind: Secret\n  metadata:\n    name: csi-rbd-secret\n    namespace: default\n  stringData:\n    userID: admin                                         # \u203c\ufe0f ChangeMe-01\n    userKey: AQC08qBirX7TLhAAn    # \u203c\ufe0f ChangeMe-02\n  EOF\n")),(0,a.kt)("p",null,"  \ud83d\udd36 applu "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"  kubectl apply -f csi-rbd-secret.yaml\n")),(0,a.kt)("p",null,"  \ud83d\udd36 check "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"  kubectl get secret\n  NAME             TYPE     DATA   AGE\n  csi-rbd-secret   Opaque   2      37s\n")),(0,a.kt)("p",null,"==\n\ud83d\udd35 3. csi-kms-config-map.yaml    \u279c  set kms / just copy&paste     \u2705"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"cat <<EOF > csi-kms-config-map.yaml\n---\napiVersion: v1\nkind: ConfigMap\ndata:\n  config.json: |-\n    {}\nmetadata:\n  name: ceph-csi-encryption-kms-config\nEOF\n")),(0,a.kt)("p",null,"  \ud83d\udd36 apply "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"kubectl apply -f csi-kms-config-map.yaml\n")),(0,a.kt)("p",null,"\ud83d\udd35 4. ceph-csi-config.yaml       \u279c  unknow  / just copy&paste "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"cat <<EOF > ceph-config-map.yaml\n---\napiVersion: v1\nkind: ConfigMap\ndata:\n  ceph.conf: |\n    [global]\n    auth_cluster_required = cephx\n    auth_service_required = cephx\n    auth_client_required = cephx\n  # keyring is a required key and its value should be empty\n  keyring: |\nmetadata:\n  name: ceph-config\nEOF\n\n\nkubectl apply -f ceph-config-map.yaml\n")),(0,a.kt)("p",null,"\ud83d\udd35    Edit provisioner.yaml      - Option "),(0,a.kt)("p",null,"  \ud83d\udd36 WHY "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"K8s-Mgr.Root ~ kubectl get pods\nNAME                                         READY   STATUS    RESTARTS   AGE\ncsi-rbdplugin-jh4j2                          3/3     Running   0          12h\ncsi-rbdplugin-provisioner-86984584f5-f84bw   7/7     Running   0          12h\ncsi-rbdplugin-provisioner-86984584f5-qllx4   0/7     Pending   0          12h  \ncsi-rbdplugin-provisioner-86984584f5-skbx7   0/7     Pending   0          12h  \n\nby default it create 3 csi-rbdplugin-provisioner:  \n  one run. two pending.\n    i just keep one run.  no want two pending. \n      jsut need change replicas from 3 to 1 \n\n\n    \ud83d\udd36 Check  replicas \n\n      cat csi-rbdplugin-provisioner.yaml | grep replicas\n        replicas: 3\n\n    \u25ce use sed change to 1 \n      sed -i 's/replicas: 3/replicas: 1/g' csi-rbdplugin-provisioner.yaml\n\n    \u25ce check result \n      cat csi-rbdplugin-provisioner.yaml | grep replicas\n        replicas: 1\n\n\n\n\n\ud83d\udd36 debug \n\n  kubectl get namespace\n  kubectl get pods --all-namespaces\n  kubectl get events -n <namespace> \n  kubectl get events -n \n  \n")),(0,a.kt)("p",null,"\ud83d\udd35 5. install driver             (all k8s node ? )"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"kubectl create -f csi-provisioner-rbac.yaml\nkubectl create -f csi-nodeplugin-rbac.yaml\n\nkubectl create -f csi-rbdplugin-provisioner.yaml\nkubectl create -f csi-rbdplugin.yaml\n")),(0,a.kt)("p",null,"\ud83d\udd35 6. Check Status . \u2705"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"kubectl get pods\n  NAME                                         READY   STATUS    RESTARTS   AGE\n  csi-rbdplugin-provisioner-86984584f5-2vd5r   7/7     Running   0          21s\n  csi-rbdplugin-provisioner-86984584f5-b96hj   0/7     Pending   0          21s\n  csi-rbdplugin-provisioner-86984584f5-k8jlx   0/7     Pending   0          21s\n  csi-rbdplugin-r7t5b                          3/3     Running   0          16s\n")),(0,a.kt)("p",null,"==\n\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35  Ceph-CSI PVC demo"),(0,a.kt)("p",null,"\ud83d\udd35 Demo Desc "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"it.admin config storage class first.\n\ndocker user create pvc. \nstorageclass auto create pv.\n\nsave it.admin time\n")),(0,a.kt)("p",null,"\ud83d\udd35 Config Storage Class "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"\ud83d\udd36 create yaml \n\ncat <<EOF > csi-rbd-sc.yaml\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: csi-rbd-sc\nprovisioner: rbd.csi.ceph.com\nparameters:\n  clusterID: b57b2062-e75d-11ec-8f3e-45be4942c0cb      # \u203c\ufe0f  Change Me 01\n  pool: Pool_BD-K8s_Test                               # \u203c\ufe0f  Change Me 02\n  imageFeatures: layering\n  csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret\n  csi.storage.k8s.io/provisioner-secret-namespace: default\n  csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret\n  csi.storage.k8s.io/controller-expand-secret-namespace: default\n  csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret\n  csi.storage.k8s.io/node-stage-secret-namespace: default\nreclaimPolicy: Delete\nallowVolumeExpansion: true\nmountOptions:\n  - discard\nEOF\n")),(0,a.kt)("p",null,"  \ud83d\udd36 apply yaml "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"kubectl apply -f csi-rbd-sc.yaml\n")),(0,a.kt)("p",null,"  \ud83d\udd36 Check SC"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"K8s-Mgr.Root kubernetes kubectl get storageclass\nNAME         PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\ncsi-rbd-sc   rbd.csi.ceph.com   Delete          Immediate           true                   9h\n")),(0,a.kt)("p",null,"\ud83d\udd35 Create block-based PVC"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"i only use ceph-bd /rbd /image function.\nso i only need create block-based pvc.\n\nif you use ceph-fs you need create file-system-based pvc\n  use offical document \n    https://docs.ceph.com/en/latest/rbd/rbd-kubernetes/?highlight=CSI\n")),(0,a.kt)("p",null,"  \ud83d\udd36 create pvc yaml "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"cat <<EOF > raw-block-pvc.yaml\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: raw-block-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Block             # \u203c\ufe0f this is block type.  no need change\n  resources:\n    requests:\n      storage: 1Gi              # \u203c\ufe0f Only need change Size\n  storageClassName: csi-rbd-sc\nEOF\n")),(0,a.kt)("p",null,"  \ud83d\udd36 apply pvc"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"  kubectl apply -f raw-block-pvc.yaml\n")),(0,a.kt)("p",null,"  \ud83d\udd36 check pvc status "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"kubectl get pvc\nNAME            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nraw-block-pvc   Bound\u203c\ufe0f   pvc-d55e2cee-c122-4364-a2d8-d164ecadd012   1Gi        RWO            csi-rbd-sc     9h\n")),(0,a.kt)("p",null,"\ud83d\udd35 Bind PVC to pod   \u279c copy & paste"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'cat <<EOF > raw-block-pod.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-raw-block-volume\nspec:\n  containers:\n    - name: fc-container\n      image: fedora:26\n      command: ["/bin/sh", "-c"]\n      args: ["tail -f /dev/null"]\n      volumeDevices:\n        - name: data\n          devicePath: /dev/xvda\n  volumes:\n    - name: data\n      persistentVolumeClaim:\n        claimName: raw-block-pvc\nEOF\n\n\nkubectl apply -f raw-block-pod.yaml\n')),(0,a.kt)("p",null,"  \ud83d\udd36 Check pod"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"kubectl get pod\npod-with-raw-block-volume                    0/1     ImagePullBackOff   0          10m\n")),(0,a.kt)("p",null,"\ud83d\udd35 Ceph Check Image "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"now k8s can use pool in ceph.\nno need to create image manual! \n\n\ud83d\udd36 Check Ceph Pool\n\n  CEPH-MGR.Root ~ rbd -p Pool_BD-K8s_Test ls\n  IMG-K8s-Test\n  csi-vol-c8000098-edff-11ec-8899-0e9db053906f       \u279c this is k8s create for us.\n")))}u.isMDXComponent=!0}}]);