"use strict";(self.webpackChunkblog_never_del=self.webpackChunkblog_never_del||[]).push([[1113],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>m});var r=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function c(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var i=r.createContext({}),l=function(e){var n=r.useContext(i),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},p=function(e){var n=l(e.components);return r.createElement(i.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},u=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,i=e.parentName,p=c(e,["components","mdxType","originalType","parentName"]),u=l(t),m=a,k=u["".concat(i,".").concat(m)]||u[m]||d[m]||o;return t?r.createElement(k,s(s({ref:n},p),{},{components:t})):r.createElement(k,s({ref:n},p))}));function m(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var o=t.length,s=new Array(o);s[0]=u;var c={};for(var i in n)hasOwnProperty.call(n,i)&&(c[i]=n[i]);c.originalType=e,c.mdxType="string"==typeof e?e:a,s[1]=c;for(var l=2;l<o;l++)s[l]=t[l];return r.createElement.apply(null,s)}return r.createElement.apply(null,t)}u.displayName="MDXCreateElement"},6301:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>c,toc:()=>l});var r=t(7462),a=(t(7294),t(3905));const o={sidebar_position:2930,title:"\ud83c\udfaa\ud83c\udfaa\ud83d\udc2c\u2638\ufe0f\u2638\ufe0f3\ufe0f\u20e3 STO \u279c Basic"},s="Storage \u2736 PV PVC CSI",c={unversionedId:"\ud83c\udfaa\ud83c\udfaa \ud83d\udc2c\u2638\ufe0f\u2638\ufe0f-3\ufe0f\u20e3 STO \u279c  Basic",id:"\ud83c\udfaa\ud83c\udfaa \ud83d\udc2c\u2638\ufe0f\u2638\ufe0f-3\ufe0f\u20e3 STO \u279c  Basic",title:"\ud83c\udfaa\ud83c\udfaa\ud83d\udc2c\u2638\ufe0f\u2638\ufe0f3\ufe0f\u20e3 STO \u279c Basic",description:"\ud83d\udd35 K8s Storage",source:"@site/docs/\ud83c\udfaa\ud83c\udfaa \ud83d\udc2c\u2638\ufe0f\u2638\ufe0f-3\ufe0f\u20e3 STO \u279c  Basic.md",sourceDirName:".",slug:"/\ud83c\udfaa\ud83c\udfaa \ud83d\udc2c\u2638\ufe0f\u2638\ufe0f-3\ufe0f\u20e3 STO \u279c  Basic",permalink:"/\ud83c\udfaa\ud83c\udfaa \ud83d\udc2c\u2638\ufe0f\u2638\ufe0f-3\ufe0f\u20e3 STO \u279c  Basic",draft:!1,tags:[],version:"current",sidebarPosition:2930,frontMatter:{sidebar_position:2930,title:"\ud83c\udfaa\ud83c\udfaa\ud83d\udc2c\u2638\ufe0f\u2638\ufe0f3\ufe0f\u20e3 STO \u279c Basic"},sidebar:"defaultSidebar",previous:{title:"\ud83c\udfaa\ud83c\udfaa\ud83d\udc2c\u2638\ufe0f\u2638\ufe0f1\ufe0f\u20e3 Helm \u279c Demo",permalink:"/\ud83c\udfaa\ud83c\udfaa \ud83d\udc2c\u2638\ufe0f\u2638\ufe0f-1\ufe0f\u20e3 Helm \u279c Demo"},next:{title:"\ud83c\udfaa\ud83c\udfaa\ud83d\udc2c\u2638\ufe0f\u2638\ufe0f3\ufe0f\u20e3 STO \u279c Demo",permalink:"/\ud83c\udfaa\ud83c\udfaa \ud83d\udc2c\u2638\ufe0f\u2638\ufe0f-3\ufe0f\u20e3 STO \u279c\u279c Demo"}},i={},l=[],p={toc:l};function d(e){let{components:n,...t}=e;return(0,a.kt)("wrapper",(0,r.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"storage--pv-pvc-csi"},"Storage \u2736 PV PVC CSI"),(0,a.kt)("p",null,"\ud83d\udd35 K8s Storage"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"local    emptyDir   \u279c                          \u279c pod del, data del \nlocal    hostPath   \u279c  mount local  volume.    \u279c pod del, data keep.\n\nnas      nfs        \u279c  mount remote volume     \u279c pod del, data keep.     \u279c host down, data keep.\nnas      iscsi      \u279c  mount remote volume     \u279c pod del, data keep.     \u279c host down, data keep.\nnas      rbd        \u279c  mount remote volume     \u279c pod del, data keep.     \u279c host down, data keep.\n.... \n\n\nk8s support a lot storage type.\n  but ! every type of storage need different config!  not easy for use & config\n    so  bset choose is use pv: persisentvolume\n")),(0,a.kt)("p",null,"\ud83d\udd35 PV & PVC Desc \u2705"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"Pod (volume) \n    >> PVC                      \u279c  docker user config  \n        >> PV                   \u279c  storage manager config\n            >> RealStorage      \u279c  ceph cluster\n\n\nPV:   Persistent Volume          \u279c  like physical disk \nPVC:  Persistent Volume Claim    \u279c  choose which disk/pv  .  and decide the size of disk.\n\nPV:   Provide  Storage           \u279c  config pv   is  it admin`s     job\nPVC:  Use      Storage           \u279c  config pvc  is  docker user`s  job \n\n\nPV can cofing on all type of storage.\n  pv is just like a hard disk to user. \n    user just need config pvc (how to use disk. need how big & etc...)\n    user konw nothing about your real storage. \n        user shoud not know this for security. and no need to know this.\n\n\n      \n")),(0,a.kt)("p",null,"\ud83d\udd35 StorageClass / Driver / Plugin"),(0,a.kt)("p",null,"  \ud83d\udd36 WHY "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"PV & PVC make k8s storage config much easy.\n    but config pv & pvc is still not that easy. \n      it is two people`s job! \n        if you want config pvc. must let it.admin config pv first.\n\nwe have somthing better: Provisioner/StorageClass\n    Provisioner/StorageClass can auotmatic create pv when docker user create pvc.\n\nProvisioner/StorageClass is just like driver for storage cluster.\n    different storage cluster need different dirver \n")),(0,a.kt)("p",null,"  \ud83d\udd36 StorageClass / Driver _ CEPH-CSI"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"  StorageClass is not buildin.\n  we need config StorageClass First! \n    we use ceph-rbd \n    so we need install ceph-csi plugin first.\n\n    with this plugin. k8s can control ceph cluster.\n    so k8s can create/delete ceph-rbd disk on ceph.\n")),(0,a.kt)("p",null,"\ud83d\udd35 CEPH-CSI Desc"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"CSI:   Container Storage Interface\n  a dirver/plugin for container;   allow k8s control ceph cluster. \n    like NIC: make one pc connect other pc.\n    here CSI: make K8s Storage Connect Ceph Storage. \n")),(0,a.kt)("p",null,"\ud83d\udd35 How "),(0,a.kt)("p",null,"  i have a detail demo under sto.rbd.ceph-csi install\nthis is simple demo "),(0,a.kt)("p",null,"  install cephadm to all k8s node\nso k8s can support ceph storage. "),(0,a.kt)("p",null,"  install ceph-csi driver to all k8s node\nso k8s can support ceph much smart\nlike auto create pv for you ."),(0,a.kt)("p",null,"  prepair pool & user in ceph cluster. "),(0,a.kt)("p",null,"  test pvc in k8s "),(0,a.kt)("p",null,"\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35 CEPH-CSI prepair "),(0,a.kt)("p",null,"\ud83d\udd35 install cephadm tool  all node"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"sudo apt install -y cephadm \n\n  test ceph port \n    nmap -p 6789 10.12.12.70\n")),(0,a.kt)("p",null,"\ud83d\udd35 config ceph-csi plugin/driver in all node"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"git clone --depth 1 --branch devel https://github.com/ceph/ceph-csi.git\n\ncd /root/ceph-csi/deploy/rbd/kubernetes\n")),(0,a.kt)("p",null,"\ud83d\udd36 1. edit csi-config-map.yaml"),(0,a.kt)("p",null,"  \u203c\ufe0f only change id&ip \u203c\ufe0f\n\u203c\ufe0f only change id&ip \u203c\ufe0f\n\u203c\ufe0f only change id&ip \u203c\ufe0f"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'cat <<EOF > csi-config-map.yaml\n---\napiVersion: v1\nkind: ConfigMap\ndata:\n  config.json: |-\n    [\n      {\n        "clusterID": "b57b2062-e75d-11ec-8f3e-45be4942c0cb",   # \u203c\ufe0f ChanegMe-01 \n        "monitors": [\n          "10.12.12.70:6789"                                   # \u203c\ufe0f ChangeMe-02\n        ]\n      }\n    ]\nmetadata:\n  name: ceph-csi-config\nEOF\n')),(0,a.kt)("p",null,"  kubectl apply -f csi-config-map.yaml\nkubectl get configmap"),(0,a.kt)("p",null,"\ud83d\udd36 2.  csi-rbd-secret.yaml        \u279c  set Ceph Username & password  \u2705"),(0,a.kt)("p",null,"  \u203c\ufe0f try use ceph admin if possible .  other user have unknow problem "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"cat <<EOF > csi-rbd-secret.yaml\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: csi-rbd-secret\n  namespace: default\nstringData:\n  userID: admin                                         # \u203c\ufe0f ChangeMe-01\n  userKey: AQC08qBirxxxxx=     # \u203c\ufe0f ChangeMe-02\nEOF\n\n\n\nkubectl apply -f csi-rbd-secret.yaml\nkubectl get secret\n")),(0,a.kt)("p",null,"\ud83d\udd36 3. csi-kms-config-map.yaml    \u279c  set kms / just copy&paste     \u2705"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"cat <<EOF > csi-kms-config-map.yaml\n---\napiVersion: v1\nkind: ConfigMap\ndata:\n  config.json: |-\n    {}\nmetadata:\n  name: ceph-csi-encryption-kms-config\nEOF\n\n\n\nkubectl apply -f csi-kms-config-map.yaml\n")),(0,a.kt)("p",null,"\ud83d\udd36 4. ceph-csi-config.yaml       \u279c  unknow  / just copy&paste "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"cat <<EOF > ceph-config-map.yaml\n---\napiVersion: v1\nkind: ConfigMap\ndata:\n  ceph.conf: |\n    [global]\n    auth_cluster_required = cephx\n    auth_service_required = cephx\n    auth_client_required = cephx\n  # keyring is a required key and its value should be empty\n  keyring: |\nmetadata:\n  name: ceph-config\nEOF\n\n\n  kubectl apply -f ceph-config-map.yaml\n")),(0,a.kt)("p",null,"\ud83d\udd36 5. Edit provisioner.yaml      - Option "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"by default it create 3 same pod for backup!\nwe k3s for test. only need 1. \n\n\n  \ud83d\udd3b Check  replicas \n    cat csi-rbdplugin-provisioner.yaml | grep replicas\n      replicas: 3\n\n  \u25ce use sed change to 1 \n    sed -i 's/replicas: 3/replicas: 1/g' csi-rbdplugin-provisioner.yaml\n\n  \u25ce check result \n    cat csi-rbdplugin-provisioner.yaml | grep replicas\n      replicas: 1\n")),(0,a.kt)("p",null,"\ud83d\udd36 apply other. "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"kubectl create -f csi-provisioner-rbac.yaml\nkubectl create -f csi-nodeplugin-rbac.yaml\n\nkubectl create -f csi-rbdplugin-provisioner.yaml\nkubectl create -f csi-rbdplugin.yaml\n")),(0,a.kt)("p",null,"\ud83d\udd36 check status\nnow. ceph-csi is done. you can test.it.\nthis need take some time to finisth."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"kubectl get pods\nNAME                                         READY   STATUS    RESTARTS   AGE\ncsi-rbdplugin-xp84x                          3/3     Running   0          2m47s\ncsi-rbdplugin-7bj26                          3/3     Running   0          2m47s\ncsi-rbdplugin-provisioner-5d969665c5-2gvbq   7/7     Running   0          2m51s\n")),(0,a.kt)("p",null,"\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35 ceph  prepair "),(0,a.kt)("p",null,"\ud83d\udd35 ceph  prepair "),(0,a.kt)("p",null,"  \ud83d\udd36 Create Ceph K3s Pool:    CEPH_BD-K3s"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"  ceph osd lspools\n  ceph osd pool create CEPH_BD-K3s 128 128 \n  ceph osd pool set CEPH_BD-K3s size 1\n  ceph osd pool set CEPH_BD-K3s target_size_bytes 1000G\n\n  sudo rbd pool init CEPH_BD-K3s\n  rbd ls CEPH_BD-K3s\n")),(0,a.kt)("p",null,"  \ud83d\udd36 Create Ceph K3s User : k3s "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"ceph auth get-or-create client.k3s mon 'allow r' osd 'allow rw pool=CEPH_BD-K3s'\n\n    [client.k3s]\n            key = AQAJcctixrxxxx\n")),(0,a.kt)("p",null,"  \ud83d\udd36 get admin key          \u279c     ceph auth get client.admin"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"    key = AQC08qBixxxx09AX7TtstKNAA==\n")),(0,a.kt)("p",null,"  \ud83d\udd36 get cluster id / fsid  \u279c     ceph mon dump\nfsid b57b2062-e75d-11ec-8f3e-45be4942c0cb"),(0,a.kt)("p",null,"\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35\ud83d\udd35 k8s pvc demo "),(0,a.kt)("p",null,"\ud83d\udd35 Config StorageClass \u2705"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'  cat <<EOF > csi-config-map.yaml\n  ---\n  apiVersion: v1\n  kind: ConfigMap\n  data:\n    config.json: |-\n      [\n        {\n          "clusterID": "b57b2062-e75d-11ec-8f3e-45be4942c0cb",   # \u203c\ufe0f ChanegMe-01 \n          "monitors": [\n            "10.12.12.70:6789"                                   # \u203c\ufe0f ChangeMe-02\n          ]\n        }\n      ]\n  metadata:\n    name: ceph-csi-config\n  EOF\n\n\ncat <<EOF > sto-sc-cephcsi-rbd-k3s.yaml\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: sto-sc-cephcsi-rbd-k3s                          # \u203c\ufe0f Change Me 03\nprovisioner: rbd.csi.ceph.com\nparameters:\n  clusterID: b57b2062-e75d-11ec-8f3e-45be4942c0cb      # \u203c\ufe0f  Change Me 01\n  pool: CEPH_BD-K3s                               # \u203c\ufe0f  Change Me 02\n  imageFeatures: layering\n  csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret\n  csi.storage.k8s.io/provisioner-secret-namespace: default\n  csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret\n  csi.storage.k8s.io/controller-expand-secret-namespace: default\n  csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret\n  csi.storage.k8s.io/node-stage-secret-namespace: default\nreclaimPolicy: Delete\nallowVolumeExpansion: true\nmountOptions:\n  - discard\nEOF\n\n\n\nkubectl apply -f sto-sc-cephcsi-rbd-k3s.yaml\n')),(0,a.kt)("p",null,"\ud83d\udd35 pvc create demo"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"cat <<EOF > pvc-k3s-db-mysql.yaml\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-k3s-db-mysql                         # \u203c\ufe0f Must         set pvc name\nspec:\n  accessModes:\n    - ReadWriteOnce                         # \u203c\ufe0f ceph available Option: ReadWriteOnce/ReadOnlyMany\n  volumeMode: Block                         # \u203c\ufe0f NoChange:    this is block type. \n  resources:\n    requests:\n      storage: 500Gi                         # \u203c\ufe0f Option       Change Size\n  storageClassName: sto-sc-cephcsi-rbd-k3s   # \u203c\ufe0f Must         Chaneg To your sc name \nEOF\n\n\n\n\nkubectl apply -f pvc-k3s-db-mysql.yaml\n")),(0,a.kt)("p",null,"\ud83d\udd35 PVC debug "),(0,a.kt)("p",null,"  \ud83d\udd36 check sc pv pvc status "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"kubectl get sc\nkubectl get pv\nkubectl get pvc    \u279c  if ok. it should bond to sc!   \n\n\nK3s ~ kubectl get pvc\nNAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS             AGE\nsto-pvc-k3s   Pending \u274c                                     sto-sc-cephcsi-rbd-k3s   37m\n")),(0,a.kt)("p",null,"  \ud83d\udd36 kubectl describe pvc"),(0,a.kt)("p",null,"\ud83d\udd35 pvc use demo"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'cat <<EOF > pod-netdebug.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-netdebug                       # \u203c\ufe0f set pod name \nspec:\n  containers:\n    - name: nettools                        # \u203c\ufe0f any name\n      image: travelping/nettools            # \u203c\ufe0f image url must right\n      command:                              # \u203c\ufe0f must run something. ro docker die.\n        - sleep\n        - "infinity"\n      volumeDevices:\n        - name: ceph-k3s-pod-nettools        # \u203c\ufe0f any name... same with blow: volumes.name\n          devicePath: /tmp                   # \u203c\ufe0f container inside  path\n  volumes:\n    - name: ceph-k3s-pod-nettools            # \u203c\ufe0f any name... same wuith up:  volumeDevices.name\n      persistentVolumeClaim:\n        claimName: sto-pvc-k3s               # \u203c\ufe0f use your pvc name\nEOF\n\n\nkubectl apply -f pod-netdebug.yaml\n')),(0,a.kt)("p",null,"\ud83d\udd35 check ceph. "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"CEPH-MGR.Root ~ rbd -p Pool_BD-K8s_Prod ls\nIMG-K8s-Prod\nIMG-K8s-Prod-NoCSI\ncsi-vol-59b173b3-ee1b-11ec-8899-0e9db053906f\n\n\n\u203c\ufe0f deleted pvc delete disk in ceph;   delete pod. disk still keep \u203c\ufe0f\n\u203c\ufe0f deleted pvc delete disk in ceph;   delete pod. disk still keep \u203c\ufe0f\n\u203c\ufe0f deleted pvc delete disk in ceph;   delete pod. disk still keep \u203c\ufe0f\n\npvc is like disk.  you can mount all pod folder into one pvc. \nbut never delete pvc.\n\nbut. how to create folder. in pvc? \nhow my k8s node can visit pvc folder.\n")))}d.isMDXComponent=!0}}]);