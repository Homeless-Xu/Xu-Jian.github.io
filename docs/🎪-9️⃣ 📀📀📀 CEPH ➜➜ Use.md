---
sidebar_position: 1930
title: ğŸª-9ï¸âƒ£ğŸ“€ğŸ“€ğŸ“€ CEPH âœâœ Use
---

# # Storage âœ¶ CEPH âœ Use



ğŸ”µ Good File 

â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸
https://zhangzhuo.ltd/articles/2021/06/18/1623993444076.html


â­ï¸â­ï¸â­ï¸â­ï¸
https://www.koenli.com/ef5921b8.html



https://blog.51cto.com/renlixing/3134294


ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ Basic Info 
ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ Basic Info
ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ Basic Info

ğŸ”µ POOL PG PGP Desc  

    ğŸ”¶ Pool 

        ceph osd pool create poolname PG PGP 
        ceph osd pool create mytestpool 128 128 

        åˆ›å»ºpool æ—¶å€™å¿…é¡»æŒ‡å®š PG æ•°é‡.
        PG æ•°é‡ æ˜¯è‡ªåŠ¨åˆ†å¸ƒåˆ° æ‰€æœ‰ OSDçš„.
        æ‰€ä»¥ä½ çš„ Pool ä¹Ÿæ˜¯åˆ†å¸ƒåˆ°æ•´ä¸ª é›†ç¾¤çš„.



    ğŸ”¶ PGs Desc  âœ…

        ceph cluster have lots physical disk.
            to manager disk easyier. we create PGs ( assign physical disk to some group )
                so we only need manage PGs.



    ğŸ”¶ PG / PGP 

        PG è¦æ ¹æ® OSD æ•°é‡è¿›è¡Œè°ƒæ•´.
        PGP å’Œ PG çš„å€¼ä¿æŒä¸€è‡´å°±è¡Œ.
        æ”¹PG çš„åŒæ—¶è¦ PGP ä¸€èµ·æ”¹.


        å°‘äº50 osd å¯ä»¥ç”¨è‡ªåŠ¨æ–¹å¼. å†å¤šå°±è¦è‡ªå·±ç®—äº†. 
            pg_autoscale_mode: on   å°±å¯ä»¥äº†

    

ğŸ”µ Storage Develop 

    DAS:  Dirtect Attached Storage 
        âœ NAS:  Network Attached Storage 
            âœ SAN:  Storage Area Network 
                âœ Object Storage



ğŸ”µ CEPH Function:

    Object Storage âœ like AWS S3    âœ For Share              âœ high speed 
    Block  Storage âœ like iscsi     âœ For One Device Only    âœ Fast Speed 
    File   Storage âœ like NFS/SMB   âœ For Share              âœ Slow Speed

        object storage have both advantage of  block storage and file storage.
            if data that don`t change too often.    âœ Object storage is best.  like picture/video 
            if you install VM                       âœ Block Storage is better.



 

ğŸ”µ CEPH FIle  Storage Workflow

    1. File   >> Object           cut big file to samll object(2M-4M)
        2. Object >> PG           give small object a PGs.
            3. PG     >> OSD      Write  small object to OSD


 
ğŸ”µ NEW OSD 

    when add new driver.
    it will automatic move date between osdes. 



ğŸ”µ Pool Commands 

    ğŸ”¶ Pool 

        â— Create pool     ceph osd pool create CEPH_FS-APP 128 128 
        â— Rename Pool     ceph osd pool rename CEPH-Test CEPH_FS-NAS-Ceph
        â— List   pool     ceph osd lspools
        â— Delete Pool




    ğŸ”¶ AutoScale Mode  

        â— Set Mode On    ceph osd pool set CEPH-Test pg_autoscale_mode on
        â— Check Mode     ceph osd pool autoscale-status
            default: on 


    ğŸ”¶ Pool Size 

        ceph osd pool set CEPH-Test target_size_bytes 10G



    ğŸ”¶ Pool replica size 

        ceph osd pool set CEPH-Test size 2
        
        default is 3 
        you can change this anytime.
        if you have space set 3/2
        if no space set to 1 at any time.


    ğŸ”¶ Delete Pool âœ…

        â— Allow Delete pool 
            by default it is not allowed.

            ceph tell mon.\* injectargs '--mon-allow-pool-delete=true'


        â— How Delete 

            ceph osd pool delete {pool-name} {pool-name} --yes-i-really-really-mean-it
                type pool name twice. 
                    and follew --yes-i-really-really-mean-it

            ceph osd pool delete test-pool test-pool --yes-i-really-really-mean-it

            ceph osd pool delete OB-Zone-Ark.rgw.meta OB-Zone-Ark.rgw.meta --yes-i-really-really-mean-it




ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ Filesystem Demo 
ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ Filesystem Demo 
ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ Filesystem Demo 

ğŸ”µ Server âœ¶ Pool Config 

    ğŸ”¶ Create Pool for cephfs 

        ceph osd pool create CEPH_FS-NAS-Ceph 128 128 
        ceph osd pool create CEPH_FS-NAS-Ceph-metadata 128 128 


    ğŸ”¶ Assign Pool Type to Filesystem Storage

        â— Desc 
            sudo ceph osd pool application enable PoolNAME StorageType
                ceph have tree storage type: file Type /Block Devie Type /Object Type.
                so you need set pool use which type.
                here cephfs type.
     
        â— Demo 
            sudo ceph osd pool application enable CEPH_FS-NAS-Ceph cephfs
            sudo ceph osd pool application enable CEPH_FS-NAS-Ceph-metadata cephfs


    ğŸ”¶ Create fs:  cephfs

        ceph fs new cephfs CEPH_FS-NAS-Ceph-metadata CEPH_FS-NAS-Ceph


    ğŸ”¶ Check fs status

        CEPH-MGR.Root ~ ceph fs ls
        name: cephfs, metadata pool: CEPH_FS-NAS-Ceph-metadata, data pools: [CEPH_FS-NAS-Ceph ]


    ğŸ”¶ assign cephfs to node

        ceph orch apply mds cephfs --placement="1  CEPH-Node"


    ğŸ”¶ check mds status 

        CEPH-MGR.Root ~ ceph orch ps --daemon-type mds
        NAME                         HOST       STATUS         REFRESHED  AGE  VERSION  IMAGE NAME             IMAGE ID      CONTAINER ID
        mds.cephfs.CEPH-Node.tevxqv  CEPH-Node  running (33m)  4m ago     33m  15.2.16  quay.io/ceph/ceph:v15  8d5775c85c6a  6e5f7903011d


ğŸ”µ Server âœ¶ User Create 

    ğŸ”¶ Create user for ceph-fs mount

        ceph auth get-or-create client.miranda mon 'allow r' mds 'allow r, allow rw path=/' osd 'allow rw pool=data' -o ceph.client.miranda.keyring

                 â— username:  client.miranda

            â€¼ï¸ cd /etc/ceph/  firts.   user name must be like  client.xxxx /  only change miranda above. leave else â€¼ï¸ 
            â€¼ï¸ cd /etc/ceph/  firts.   user name must be like  client.xxxx /  only change miranda above. leave else â€¼ï¸ 
    

    ğŸ”¶ cat user token (option)

        ceph auth get-key client.miranda




ğŸ”µ Client âœ¶ Mount (Ceph-fuse)

        â€¼ï¸ ceph-fuse is like cephadm. no ububtu22 now. use ubuntu 20! â€¼ï¸ 
        â€¼ï¸ ceph-fuse is like cephadm. no ububtu22 now. use ubuntu 20! â€¼ï¸ 
        â€¼ï¸ ceph-fuse is like cephadm. no ububtu22 now. use ubuntu 20! â€¼ï¸ 


    ğŸ”¶ Insatll ceph-fuse 

        â— Ubuntu 20:   sudo apt install ceph-fuse  

    ğŸ”¶ Make dir 

        sudo mkdir -p /etc/ceph


    ğŸ”¶ Copy Moniter ceph.conf  to Client 

        sudo scp {user}@{server-machine}:/etc/ceph/ceph.conf /etc/ceph/ceph.conf
        sudo scp root@10.12.12.70:/etc/ceph/ceph.conf /etc/ceph/ceph.conf

            monitor node need enable root user & allow root ssh 


    ğŸ”¶ Copy Moniter xxxx.keyring  to Client

        sudo scp {user}@{server-machine}:/etc/ceph/ceph.keyring /etc/ceph/ceph.keyring
        sudo scp root@10.12.12.70:/etc/ceph/ceph.client.miranda.keyring /etc/ceph/fs-miranda.keyring
            
            this download keyring  and renamed to fs-miranda.keyring 


    ğŸ”¶ Make mount Folder 

        sudo mkdir /mnt/ceph-cephfs-MountPoint


    ğŸ”¶ Mount cephfs 

        ceph-fuse [-n client.username] [ -m monaddr:port ] mountpoint [ fuse options ]
        sudo ceph-fuse -n client.miranda -m 10.12.12.70:6789 /mnt/ceph-cephfs-MountPoint


    ğŸ”¶ check nfs 

        mount -l | grep ceph



ğŸ”µ Client âœ¶ Auto Mount - ceph fs  ğŸ’¯

    ğŸ”¶ Create Script 

        sudo bash -c 'echo "
        #!/bin/bash
        ceph-fuse -n client.miranda -m 10.12.12.70:6789 /mnt/ceph-cephfs-MountPoint
        " > /root/crontab-script-ceph-fs-Mount-NoDEL.sh'


    ğŸ”¶ Give Perssmion

            chmod +x /root/crontab-script-ceph-fs-Mount-NoDEL.sh



    ğŸ”¶ crontab desc 

        â¦¿  crontab -l   check  task
        â¦¿  crontab -e   edit/add task
        â¦¿  crontab -r   remove  task 


    ğŸ”¶ enter crontab edit 

        crontab -e


    ğŸ”¶ add a line & save file 

        @reboot /root/crontab-script-ceph-fs-Mount-NoDEL.sh


    ğŸ”¶ reboot tast 


 
 









===============================================
ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µBlock Device Demo
ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µBlock Device Demo
ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µBlock Device Demo

https://documentation.suse.com/zh-cn/ses/7/html/ses-all/cha-ceph-as-iscsi.html
??



ğŸ”µ Server âœ¶ Create Block Device Pool
 



    ğŸ”¶ create pool 

        ceph osd pool create CEPH_BD-K8s-Prod 128 128 
        ceph osd pool create CEPH_BD-K8s-Test 128 128 



    ğŸ”¶ Change repeat size to 2

        ceph osd pool set CEPH_BD-K8s-Prod size 2
        ceph osd pool set CEPH_BD-K8s-Test size 2



    ğŸ”¶ Init Pool 

        sudo rbd pool init CEPH_BD-K8s-Prod
        sudo rbd pool init CEPH_BD-K8s-Test 

            Filesystem and Object Gateway  No need Initialize 
            only Block Device need use rbd to initialize  


    ğŸ”¶ list pool 

        ceph osd lspools


======================================

ğŸ”µ Server âœ¶ Create User for Mount 

    â€¼ï¸ By default Block Device Storage Use admin account to mount, No Need Create User. â€¼ï¸
    â€¼ï¸ By default Block Device Storage Use admin account to mount, No Need Create User. â€¼ï¸
    â€¼ï¸ By default Block Device Storage Use admin account to mount, No Need Create User. â€¼ï¸


    ğŸ”¶ options 

        ceph auth get-or-create client.BD-User01 mon 'profile rbd' osd 'profile {profile name} [pool={pool-name}][, profile ...]' mgr 'profile rbd [pool={pool-name}]'

        â— read only user 
        ceph auth get-or-create client.BD-K8s-Prod-User01 mon 'profile rbd' osd 'profile rbd pool=CEPH_BD-K8s-Prod, profile rbd-read-only pool=images' mgr 'profile rbd pool=images'

        [client.BD-K8s-Prod-User01]
            key = AQCWq6FiJSG7Ah

        ceph auth get-or-create client.BD-K8s-Test-User01 mon 'profile rbd' osd 'profile rbd pool=CEPH_BD-K8s-Test, profile rbd-read-only pool=images' mgr 'profile rbd pool=images'

        [client.BD-K8s-Test-User01]
            key = AQCpq6Fiw/sJNBAAQx



========================================

ğŸ”µ Server âœ¶ Create Image âœ…

    ğŸ”¶ Image Desc 

        Pool is for ceph cluster;   Image is for Client to Mount;
            Image is like Hardware Disk let client mount & use.
                Create Image for pool_xxx  first.
                    Create User for Mount.
                        Client Mount.

        
        image is created under/inside pool. 


    ğŸ”¶ Create Image 

        rbd create --size {megabytes} {pool-name}/{image-name}

        rbd create --size 10240 CEPH_BD-K8s-Prod/IMG-K8s-Prod
        rbd create --size 10240 CEPH_BD-K8s-Test/IMG-K8s-Test

            image is under pool.
            create 10G image under pool 


    ğŸ”¶ List Image 

        rbd ls {poolname}
        rbd ls CEPH_BD-K8s-Prod
        rbd ls CEPH_BD-K8s-Test


    ğŸ”¶ Check Image info 

        rbd info {pool-name}/{image-name}
        rbd info CEPH_BD-K8s-Prod/IMG-K8s-Prod
        rbd info CEPH_BD-K8s-Test/IMG-K8s-Test



    ğŸ”¶ Resize Image  

        rbd resize --size 20480 IMG-K8s-Prod                     âœ increase size 
        rbd resize --size 1024 IMG-K8s-Test --allow-shrink       âœ decrease size  

            image only take space when use it. 
                if only create, no put data in. the real size is zero.

========================================

ğŸ”µ Client âœ¶ Mapping Image  âœ…

    ğŸ”¶ install Ceph Common

        U20-TempleOnly ~ sudo apt install ceph-common


    ğŸ”¶ Copying Conf and Keyring from moniter node

        sudo scp root@10.12.12.70:/etc/ceph/ceph.conf /etc/ceph/ceph.conf
        sudo scp root@10.12.12.70:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring

            No Change filename on client side after copy
            when mapping. it use default name to find server info.
            or next mapping will wrong 


    ğŸ”¶ mapping

        mapping is like put new disk to client server.
        if need use disk. still need format & mount.

        â— Check status    rbd showmapped 
        â— map             rbd map Pool_BD-K8s-Prod/IMG-K8s-Prod -p rbd





=====================

ğŸ”µ Client âœ¶ Mount disk    âœ…

    atter Mapping successful.
        it is like a new hardware disk.
            format it to xfs &&  mount it; 
                then we can use it.


    ğŸ”¶ check disk name 

        fdisk -l  
        Disk /dev/rbd0: 10 GiB, 10737418240 bytes, 20971520 sectors


    ğŸ”¶ Format To XFS 

        mkfs.xfs /dev/rbd0 



    ğŸ”¶ Make Mount Point 

        mkdir /mnt/ceph-img-k8s-prod


    ğŸ”¶ Mount 

        mount /dev/rbd0 /mnt/ceph-img-k8s-prod/


    ğŸ”¶ Verify 

        df -T 
        df -hl | grep rbd



ğŸ”µ Client âœ¶ Auto Mount ğŸ’¯

    ğŸ”¶ Create Script 

        sudo bash -c 'echo "
        #!/bin/bash
        rbd map Pool_BD-K8s_Prod/IMG-K8s-Prod -p rbd
        mount /dev/rbd0 /mnt/ceph-img-k8s-prod/
        " > /root/crontab-script-ceph-DB-Mount-NoDEL.sh'


        ğŸ”¶ Give Perssmion â€¼ï¸

            chmod +x /root/crontab-script-ceph-DB-Mount-NoDEL.sh



    ğŸ”¶ crontab desc 

        â¦¿  crontab -l   check  task
        â¦¿  crontab -e   edit/add task
        â¦¿  crontab -r   remove  task 


    ğŸ”¶ enter crontab edit 

        crontab -e


    ğŸ”¶ add a line & save file 

        @reboot /root/crontab-script-ceph-DB-Mount-NoDEL.sh


    ğŸ”¶ reboot tast 




ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ
ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ  OB Demo  
ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ 
âŒâŒâŒ Give Up.  can not enable fuck serverice..



â€¼ï¸ Offical manual ..

https://docs.ceph.com/en/pacific/radosgw/pools/


radosgw  commond 
https://www.mankier.com/8/radosgw-admin


ğŸ”µ AWS S3 Desc  

    Amazon Simple Storage Service

    offer a web.  in web you can control all.
    every file have a url. 
    you can use it in your web. 
    good for website!!! 



ğŸ”µ Basic info 

    rgw-us-east:
    realm: replicated
    zonegroup: us
    zone: us-east
    rgw-us-west:
    realm: replicated
    zonegroup: us
    zone: us-west



    zone + zone ... >> Zone Group 
        zone group + zone group + ... >> realm 

    all zone under the same zone group  can sync between zone.
        this make backup between two physical place possible.
            like Zone-Group-Global
                zone_1 put in china 
                zone_2 put in Us
                ...



    â€¼ï¸ no commit no take effeci:             radosgw-admin period update --commit
    â€¼ï¸ no commit no take effeci:             radosgw-admin period update --commit
    â€¼ï¸ no commit no take effeci:             radosgw-admin period update --commit



 
 
ğŸ”¶ Add placement target....
 
    $ radosgw-admin zonegroup placement add \
        --rgw-zonegroup OB-Group-Ark \
        --placement-id temporary


    $ radosgw-admin zone placement add \
        --rgw-zone OB-Zone-Ark \
        --placement-id temporary \
        --data-pool default.rgw.temporary.data \
        --index-pool default.rgw.temporary.index \
        --data-extra-pool default.rgw.temporary.non-ec



ğŸ”µ 

ğŸ”¶ Create realm:   

    radosgw-admin realm create --rgw-realm=OB-Realm-Ark --default 


ğŸ”¶ Create zone group:  

    radosgw-admin zonegroup create --rgw-zonegroup=OB-Group-Ark --master --default


ğŸ”¶ Create zone:  

    radosgw-admin zone create --rgw-zonegroup=OB-Group-Ark --rgw-zone=OB-Zone-Ark --master --default


ğŸ”¶ update period

    radosgw-admin period update --rgw-realm=OB-Realm-Ark --commit 



ğŸ”¶ Create RGW  

    orch apply rgw <realm_name> <zone_name> 
        ceph orch apply rgw OB-Realm-Ark OB-Zone-Ark --placement="1 CEPH-Node"


ğŸ”¶ Check rgw 

    ceph orch ps --daemon-type rgw
    No daemons reported

    ğŸ web:   No RGW service is running.

    



ğŸ”¶ Create USER  âŒ

    radosgw-admin user create --uid=rgw-user --display-name=rgw-user --system


    "user_id": "rgw-user",
    "display_name": "rgw-user",

            "user": "rgw-user",
            "access_key": "XD0QFJQCEJMSLEYQACGS",
            "secret_key": "WboesDxdzMezAMwQd85xpL26BAckiJfCvlZpt0Ci"
   


    è®°å½•access_keyå’Œsecret_keyçš„å€¼ä¿å­˜ä¸º access_key.txt å’Œsecret_key.txt ï¼Œé€šè¿‡å‘½ä»¤é›†æˆåˆ°dahsboardä¸­ã€‚

    ceph dashboard set-rgw-api-access-key -i accesskey.txt 
    ceph dashboard set-rgw-api-secret-key -i secretkey.txt 



 
  


ğŸ”µ OB  client ..

    # å®‰è£… AWS s3 API
    [root@ceph01 ~]# yum -y install s3cmd

    # åˆ›å»ºç”¨æˆ·
    [root@ceph01 ~]# radosgw-admin user create --uid=s3 --display-name="objcet storage" --system

    # è·å–ç”¨æˆ· access_key å’Œ secret_key
    [root@ceph01 ~]# radosgw-admin user info --uid=s3 | grep -E "access_key|secret_key"
                "access_key": "RPRUFOWDK0x",
                "secret_key": "32efWJ7OKx"

    # ç”Ÿæˆ S3 å®¢æˆ·ç«¯é…ç½®(è®¾ç½®ä¸€ä¸‹å‚æ•°, å…¶ä½™é»˜è®¤å³å¯)
    [root@ceph01 ~]# s3cmd --configure
    Access Key: RPRUFOx
    Secret Key: 32efWJ7OeKJx
    S3 Endpoint [s3.amazonaws.com]: ceph01
    DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket).ceph01
    Use HTTPS protocol [Yes]: no
    Test access with supplied credentials? [Y/n] y
    Save settings? [y/N] y
    Configuration saved to '/root/.s3cfg'

    # åˆ›å»ºæ¡¶
    [root@ceph01 ~]# s3cmd mb s3://bucket
    Bucket 's3://bucket/' created

    # æŸ¥çœ‹å½“å‰æ‰€æœ‰æ¡¶
    [root@ceph01 ~]# s3cmd ls
    2020-06-28 03:02  s3://bucket
    -----------------------------------
    Â©è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ï¼šæ¥è‡ª51CTOåšå®¢ä½œè€…çº¢å°˜ä¸–é—´çš„åŸåˆ›ä½œå“ï¼Œè¯·è”ç³»ä½œè€…è·å–è½¬è½½æˆæƒï¼Œå¦åˆ™å°†è¿½ç©¶æ³•å¾‹è´£ä»»
    ä½¿ç”¨cephadmå¿«é€Ÿæ­å»ºcephé›†ç¾¤
    https://blog.51cto.com/hongchen99/2507660






 











ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ
ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ
ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ ceph auth / user permit 

ğŸ”µ WHY CephX 

    both ceph(server) and k8s(client) have a same key.
    use this can avoid hacker atteck. 

    if any client need use ceph.
    ceph have to create use first.
    then ceph will create a username & a key.

      [client.k8s]
          key = AQBTvaZi0HhkABAAgn 


ğŸ”µ CephX Auth 

    CephX manager whole ceph cluster login system.

    inside ceph:  
        not only user(clietn.admin .. ) need password. 
        mon/osd/mds need password to login ceph too.


    outside ceph: (like k8s cluster) 
        if k8s want connect/use ceph cluster must need four info.
            â— ceph cluster id:   fsid 
            â— ceph cluster monitor ip address. 
            â— username 
            â— password 


    we use mon node to build ceph cluster.
    so even you lost client.admin password. 
    mon node can reset password for admin.



ğŸ”µ permit analyze  

    CEPH-MGR.Root ~ ceph auth ls

    client.admin
        key: AQC08qBirX7TLhAAn
        caps: [mds] allow *
        caps: [mgr] allow *
        caps: [mon] allow *
        caps: [osd] allow *

    CEPH-MGR.Root ~ ceph auth ls
    client.k8s
        key: AQBTvaZi0HhkABAAgYs 
        caps: [mgr] profile rbd pool=Pool_BD-K8s_Test
        caps: [mon] profile rbd
        caps: [osd] profile rbd pool=Pool_BD-K8s_Test

    client.miranda
        key: AQC8nKFiQgA+A
        caps: [mds] allow r, allow rw path=/
        caps: [mon] allow r
        caps: [osd] allow rw pool=data


 
    ğŸ”¶ permit 

        *  = rwx
        r:   allow read 
        w:   allow write 
        x:   allow run auth commonds (ceph auth list / ceph guth get)


    ğŸ”¶ [mon] / [mgr] / [mds]  / [osd]

        â— caps:  capabilities 

        ceph cluster have a lot role.
        differnet role have differnet job/function.
        if you need give someone ceph`s permission.
        need add role permit first.  then set detail permit under that role. 

            caps: [osd] allow *               âœ have all control to osd role.
            caps: [osd] allow rw pool=data    âœ only read+wirte under data pool. 
            

    ğŸ”¶ profile rbd pool=Pool_BD-K8s_Test




ğŸ”µ Set Permit Demo 

https://www.shuzhiduo.com/A/mo5k4alQdw/



ğŸ”µ Create/get User 

    ceph auth get-or-create client.k8s mon 'allow r' osd 'allow rw pool=Pool_BD-K8s_Test'


ğŸ”µ Change User Permit âœ…

    ceph auth get client.k8s

    ceph auth caps client.k8s mon 'allow r' osd 'allow rw pool=Pool_BD-K8s_Test'    âœ rw  one pool
    ceph auth caps client.k8s mon 'allow r' osd 'allow *'                           âœ rw  all pool 




