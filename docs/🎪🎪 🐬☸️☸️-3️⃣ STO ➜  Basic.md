---
sidebar_position: 2930
title: ğŸªğŸªğŸ¬â˜¸ï¸â˜¸ï¸3ï¸âƒ£ STO âœ Basic
---


# Storage âœ¶ PV PVC CSI




 
ğŸ”µ StorageClass / Driver / Plugin

  ğŸ”¶ WHY 

    PV & PVC make k8s storage config much easy.
        but config pv & pvc is still not that easy. 
          it is two people`s job! 
            if you want config pvc. must let it.admin config pv first.

    we have somthing better: Provisioner/StorageClass
        Provisioner/StorageClass can auotmatic create pv when docker user create pvc.
    
    Provisioner/StorageClass is just like driver for storage cluster.
        different storage cluster need different dirver 



  ğŸ”¶ StorageClass / Driver _ CEPH-CSI

      StorageClass is not buildin.
      we need config StorageClass First! 
        we use ceph-rbd 
        so we need install ceph-csi plugin first.

        with this plugin. k8s can control ceph cluster.
        so k8s can create/delete ceph-rbd disk on ceph.



ğŸ”µ CEPH-CSI Desc

    CSI:   Container Storage Interface
      a dirver/plugin for container;   allow k8s control ceph cluster. 
        like NIC: make one pc connect other pc.
        here CSI: make K8s Storage Connect Ceph Storage. 








ğŸ”µ How 

  i have a detail demo under sto.rbd.ceph-csi install 
  this is simple demo 


  install cephadm to all k8s node 
    so k8s can support ceph storage. 


  install ceph-csi driver to all k8s node
    so k8s can support ceph much smart 
      like auto create pv for you .


  prepair pool & user in ceph cluster. 

  test pvc in k8s 





ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ CEPH-CSI prepair 


ğŸ”µ install cephadm tool  all node

    sudo apt install -y cephadm 

      test ceph port 
        nmap -p 6789 10.12.12.70



ğŸ”µ config ceph-csi plugin/driver in all node


    git clone --depth 1 --branch devel https://github.com/ceph/ceph-csi.git

    cd /root/ceph-csi/deploy/rbd/kubernetes


ğŸ”¶ 1. edit csi-config-map.yaml

  â€¼ï¸ only change id&ip â€¼ï¸
  â€¼ï¸ only change id&ip â€¼ï¸
  â€¼ï¸ only change id&ip â€¼ï¸


    cat <<EOF > csi-config-map.yaml
    ---
    apiVersion: v1
    kind: ConfigMap
    data:
      config.json: |-
        [
          {
            "clusterID": "b57b2062-e75d-11ec-8f3e-45be4942c0cb",   # â€¼ï¸ ChanegMe-01 
            "monitors": [
              "10.12.12.70:6789"                                   # â€¼ï¸ ChangeMe-02
            ]
          }
        ]
    metadata:
      name: ceph-csi-config
    EOF



  kubectl apply -f csi-config-map.yaml
  kubectl get configmap



ğŸ”¶ 2.  csi-rbd-secret.yaml        âœ  set Ceph Username & password  âœ…

  â€¼ï¸ try use ceph admin if possible .  other user have unknow problem 

    cat <<EOF > csi-rbd-secret.yaml
    ---
    apiVersion: v1
    kind: Secret
    metadata:
      name: csi-rbd-secret
      namespace: default
    stringData:
      userID: admin                                         # â€¼ï¸ ChangeMe-01
      userKey: AQC08qBirxxxxx=     # â€¼ï¸ ChangeMe-02
    EOF



    kubectl apply -f csi-rbd-secret.yaml
    kubectl get secret



ğŸ”¶ 3. csi-kms-config-map.yaml    âœ  set kms / just copy&paste     âœ…

    cat <<EOF > csi-kms-config-map.yaml
    ---
    apiVersion: v1
    kind: ConfigMap
    data:
      config.json: |-
        {}
    metadata:
      name: ceph-csi-encryption-kms-config
    EOF



    kubectl apply -f csi-kms-config-map.yaml








ğŸ”¶ 4. ceph-csi-config.yaml       âœ  unknow  / just copy&paste 

    cat <<EOF > ceph-config-map.yaml
    ---
    apiVersion: v1
    kind: ConfigMap
    data:
      ceph.conf: |
        [global]
        auth_cluster_required = cephx
        auth_service_required = cephx
        auth_client_required = cephx
      # keyring is a required key and its value should be empty
      keyring: |
    metadata:
      name: ceph-config
    EOF


      kubectl apply -f ceph-config-map.yaml







ğŸ”¶ 5. Edit provisioner.yaml      - Option 

    by default it create 3 same pod for backup!
    we k3s for test. only need 1. 


      ğŸ”» Check  replicas 
        cat csi-rbdplugin-provisioner.yaml | grep replicas
          replicas: 3

      â— use sed change to 1 
        sed -i 's/replicas: 3/replicas: 1/g' csi-rbdplugin-provisioner.yaml

      â— check result 
        cat csi-rbdplugin-provisioner.yaml | grep replicas
          replicas: 1



ğŸ”¶ apply other. 

    kubectl create -f csi-provisioner-rbac.yaml
    kubectl create -f csi-nodeplugin-rbac.yaml

    kubectl create -f csi-rbdplugin-provisioner.yaml
    kubectl create -f csi-rbdplugin.yaml




ğŸ”¶ check status 
    now. ceph-csi is done. you can test.it.
      this need take some time to finisth.

    kubectl get pods
    NAME                                         READY   STATUS    RESTARTS   AGE
    csi-rbdplugin-xp84x                          3/3     Running   0          2m47s
    csi-rbdplugin-7bj26                          3/3     Running   0          2m47s
    csi-rbdplugin-provisioner-5d969665c5-2gvbq   7/7     Running   0          2m51s




ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ ceph  prepair 

ğŸ”µ ceph  prepair 

  ğŸ”¶ Create Ceph K3s Pool:    CEPH_BD-K3s

      ceph osd lspools
      ceph osd pool create CEPH_BD-K3s 128 128 
      ceph osd pool set CEPH_BD-K3s size 1
      ceph osd pool set CEPH_BD-K3s target_size_bytes 1000G

      sudo rbd pool init CEPH_BD-K3s
      rbd ls CEPH_BD-K3s



  ğŸ”¶ Create Ceph K3s User : k3s 

    ceph auth get-or-create client.k3s mon 'allow r' osd 'allow rw pool=CEPH_BD-K3s'

        [client.k3s]
                key = AQAJcctixrxxxx


  ğŸ”¶ get admin key          âœ     ceph auth get client.admin

        key = AQC08qBixxxx09AX7TtstKNAA==


  ğŸ”¶ get cluster id / fsid  âœ     ceph mon dump
        fsid b57b2062-e75d-11ec-8f3e-45be4942c0cb




ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ k8s pvc demo 

ğŸ”µ Config StorageClass âœ…




      cat <<EOF > csi-config-map.yaml
      ---
      apiVersion: v1
      kind: ConfigMap
      data:
        config.json: |-
          [
            {
              "clusterID": "b57b2062-e75d-11ec-8f3e-45be4942c0cb",   # â€¼ï¸ ChanegMe-01 
              "monitors": [
                "10.12.12.70:6789"                                   # â€¼ï¸ ChangeMe-02
              ]
            }
          ]
      metadata:
        name: ceph-csi-config
      EOF


    cat <<EOF > sto-sc-cephcsi-rbd-k3s.yaml
    ---
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: sto-sc-cephcsi-rbd-k3s                          # â€¼ï¸ Change Me 03
    provisioner: rbd.csi.ceph.com
    parameters:
      clusterID: b57b2062-e75d-11ec-8f3e-45be4942c0cb      # â€¼ï¸  Change Me 01
      pool: CEPH_BD-K3s                               # â€¼ï¸  Change Me 02
      imageFeatures: layering
      csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
      csi.storage.k8s.io/provisioner-secret-namespace: default
      csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
      csi.storage.k8s.io/controller-expand-secret-namespace: default
      csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
      csi.storage.k8s.io/node-stage-secret-namespace: default
    reclaimPolicy: Delete
    allowVolumeExpansion: true
    mountOptions:
      - discard
    EOF



    kubectl apply -f sto-sc-cephcsi-rbd-k3s.yaml





ğŸ”µ pvc create demo

    cat <<EOF > pvc-k3s-db-mysql.yaml
    ---
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: pvc-k3s-db-mysql                         # â€¼ï¸ Must         set pvc name
    spec:
      accessModes:
        - ReadWriteOnce                         # â€¼ï¸ ceph available Option: ReadWriteOnce/ReadOnlyMany
      volumeMode: Block                         # â€¼ï¸ NoChange:    this is block type. 
      resources:
        requests:
          storage: 500Gi                         # â€¼ï¸ Option       Change Size
      storageClassName: sto-sc-cephcsi-rbd-k3s   # â€¼ï¸ Must         Chaneg To your sc name 
    EOF




    kubectl apply -f pvc-k3s-db-mysql.yaml




ğŸ”µ PVC debug 

  ğŸ”¶ check sc pv pvc status 

    kubectl get sc
    kubectl get pv
    kubectl get pvc    âœ  if ok. it should bond to sc!   


    K3s ~ kubectl get pvc
    NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS             AGE
    sto-pvc-k3s   Pending âŒ                                     sto-sc-cephcsi-rbd-k3s   37m


  ğŸ”¶ kubectl describe pvc





ğŸ”µ pvc use demo


    cat <<EOF > pod-netdebug.yaml
    ---
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-netdebug                       # â€¼ï¸ set pod name 
    spec:
      containers:
        - name: nettools                        # â€¼ï¸ any name
          image: travelping/nettools            # â€¼ï¸ image url must right
          command:                              # â€¼ï¸ must run something. ro docker die.
            - sleep
            - "infinity"
          volumeDevices:
            - name: ceph-k3s-pod-nettools        # â€¼ï¸ any name... same with blow: volumes.name
              devicePath: /tmp                   # â€¼ï¸ container inside  path
      volumes:
        - name: ceph-k3s-pod-nettools            # â€¼ï¸ any name... same wuith up:  volumeDevices.name
          persistentVolumeClaim:
            claimName: sto-pvc-k3s               # â€¼ï¸ use your pvc name
    EOF


    kubectl apply -f pod-netdebug.yaml





ğŸ”µ check ceph. 

    CEPH-MGR.Root ~ rbd -p Pool_BD-K8s_Prod ls
    IMG-K8s-Prod
    IMG-K8s-Prod-NoCSI
    csi-vol-59b173b3-ee1b-11ec-8899-0e9db053906f


    â€¼ï¸ deleted pvc delete disk in ceph;   delete pod. disk still keep â€¼ï¸
    â€¼ï¸ deleted pvc delete disk in ceph;   delete pod. disk still keep â€¼ï¸
    â€¼ï¸ deleted pvc delete disk in ceph;   delete pod. disk still keep â€¼ï¸

    pvc is like disk.  you can mount all pod folder into one pvc. 
    but never delete pvc.

    but. how to create folder. in pvc? 
    how my k8s node can visit pvc folder. 





